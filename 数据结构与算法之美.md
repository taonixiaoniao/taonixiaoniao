## 01|为什么要学习数据结构和算法?
你是不是觉得数据结构和算法，跟操作系统、计算机⽹络⼀样，是脱离实际⼯作的知识？可能除了⾯试，这辈⼦也⽤不着？

尽管计算机相关专业的同学在⼤学都学过这⻔课程，甚⾄很多培训机构也会培训这⽅⾯的知识，但是据我了解，很多程序员对数据结构和算法依旧⼀窍不通。还有⼀些⼈也只听说过数组、链表、快排这些最最基本的数据结构和算法，稍微复杂⼀点的就完全没概念。

当然，也有很多⼈说，⾃⼰实际⼯作中根本⽤不到数据结构和算法。所以，就算不懂这块知识，只要Java API、开发框架⽤得熟练，照样可以把代码写得“⻜”起来。事实真的是这样吗？今天我们就来详细聊⼀聊，为什么要学习数据结构和算法。 

**想要通关⼤⼚⾯试，千万别让数据结构和算法拖了后腿**

很多⼤公司，⽐如BAT、Google、Facebook，⾯试的时候都喜欢考算法、让⼈现场写代码。有些⼈虽然技术不错，但每次去⾯试都会“跪”在算法上，很是可惜。那你有没有想过，为什么这些⼤公司都喜欢考算法呢？

校招的时候，参加⾯试的学⽣通常没有实际项⽬经验，公司只能考察他们的基础知识是否牢固。社招就更不⽤说了，越是厉害的公司，越是注重考察数据结构与算法这类基础知识。相⽐短期能⼒，他们更看中你的⻓期潜⼒。

你可能要说了，我不懂数据结构与算法，照样找到了好⼯作啊。那我是不是就不⽤学数据结构和算法呢？当然不是，你别忘了，***我们学任何知识都是为了“⽤”的，是为了解决实际⼯作问题的，学习数据结构和算法⾃然也不例外。***

**业务开发⼯程师，你真的愿意做⼀辈⼦CRUD boy吗？**

如果你是⼀名业务开发⼯程师，你可能要说，我整天就是做数据库CRUD（增删改查），哪⾥⽤得到数据结构和算法啊？

是的，对于⼤部分业务开发来说，我们平时可能更多的是利⽤已经封装好的现成的接⼝、类库来堆砌、翻译业务逻辑，很少需要⾃⼰实现数据结构和算法。***但是，不需要⾃⼰实现，并不代表什么都不需要了解。***

如果不知道这些类库背后的原理，不懂得时间、空间复杂度分析，你如何能⽤好、⽤对它们？存储某个业务数据的时候，你如何知道应该⽤ArrayList，还是Linked List呢？调⽤了某个函数之后，你⼜该如何评估代码的性能和资源的消耗呢？

作为业务开发，我们会⽤到各种框架、中间件和底层系统，⽐如Spring、RPC框架、消息中间件、Redis等等。***在这些基础框架中，⼀般都揉和了很多基础数据结构和算法的设计思想。***

⽐如，我们常⽤的Key-Value数据库Redis中，⾥⾯的有序集合是⽤什么数据结构来实现的呢？为什么要⽤跳表来实现呢？为什么不⽤⼆叉树呢？

如果你能弄明⽩这些底层原理，你就能更好地使⽤它们。即便出现问题，也很容易就能定位。因此，***掌握数据结构和算法，不管对于阅读框架源码，还是理解其背后的设计思想，都是⾮常有⽤的。***

在平时的⼯作中，数据结构和算法的应⽤到处可⻅。我来举⼀个你⾮常熟悉的例⼦：如何实时地统计业务接⼝的99%响应时间？

你可能最先想到，每次查询时，从⼩到⼤排序所有的响应时间，如果总共有1200个数据，那第1188个数据就是99%的响应时间。很显然，每次⽤这个⽅法查询的话都要排序，效率是⾮常低的。但是，如果你知道“堆”这个数据结构，⽤两个堆可以⾮常⾼效地解决这个问题。 

**基础架构研发⼯程师，写出达到开源水平的框架才是你的⽬标！**

现在互联⽹上的技术⽂章、架构分享、开源项⽬满天⻜，照猫画⻁做⼀套基础框架并不难。我就拿RPC框架举例。

不同的公司、不同的⼈做出的RPC框架，架构设计思路都差不多，最后实现的功能也都差不多。但是有的⼈做出来的框架，Bug很多、性能⼀般、扩展性也不好，只能在⾃⼰公司仅有的⼏个项⽬⾥⾯⽤⼀下。⽽有的⼈做的框架可以开源到GitHub上给很多⼈⽤，甚⾄被Apache收录。为什么会有这么⼤的差距呢？

我觉得，⾼⼿之间的竞争其实就在细节。这些细节包括：你⽤的算法是不是够优化，数据存取的效率是不是够⾼，内存是不是够节省等等。这些累积起来，决定了⼀个框架是不是优秀。所以，如果你还不懂数据结构和算法，没听说过⼤O复杂度分析，不知道怎么分析代码的时间复杂度和空间复杂度，那肯定说不过去了，赶紧来补⼀补吧！

**对编程还有追求？不想被⾏业淘汰？那就不要只会写凑合能⽤的代 码！**

何为编程能⼒强？是代码的可读性好、健壮？还是扩展性好？我觉得没法列，也列不全。但是，在 我看来，***性能好坏起码是其中⼀个⾮常重要的评判标准***。但是，如果你连代码的时间复杂度、空间 复杂度都不知道怎么分析，怎么写出⾼性能的代码呢？

你可能会说，我在⼩公司⼯作，⽤户量很少，需要处理的数据量也很少，开发中不需要考虑那么多 性能的问题，完成功能就可以，⽤什么数据结构和算法，差别根本不⼤。但是你真的想“⼗年如⼀ ⽇”地做⼀样的⼯作吗？

经常有⼈说，程序员35岁之后很容易陷⼊瓶颈，被⾏业淘汰，我觉得原因其实就在此。有的⼈写代 码的时候，从来都不考虑⾮功能性的需求，只是完成功能，凑合能⽤就好；做事情的时候，也从来 没有⻓远规划，只把眼前事情做好就满⾜了。

我曾经⾯试过很多⼤龄候选⼈，简历能写⼗⼏⻚，经历的项⽬有⼏⼗个，但是细看下来，每个项⽬ 都是重复地堆砌业务逻辑⽽已，完全没有难度递进，看不出有能⼒提升。久⽽久之，⼗年的积累可 能跟⼀年的积累没有任何区别。这样的⼈，怎么不会被⾏业淘汰呢？

如果你在⼀家成熟的公司，或者BAT这样的⼤公司，⾯对的是千万级甚⾄亿级的⽤户，开发的是 TB、PB级别数据的处理系统。性能⼏乎是开发过程中时刻都要考虑的问题。⼀个简单的ArrayList、 Linked List的选择问题，就可能会产⽣成千上万倍的性能差别。这个时候，数据结构和算法的意义 就完全凸显出来了。

其实，我觉得，数据结构和算法这个东⻄，如果你不去学，可能真的这辈⼦都⽤不到，也感受不到 它的好。但是⼀旦掌握，你就会常常被它的强⼤威⼒所折服。之前你可能需要费很⼤劲⼉来优化的 代码，需要花很多⼼思来设计的架构，⽤了数据结构和算法之后，很容易就可以解决了。 

**内容⼩结**

我们学习数据结构和算法，并不是为了死记硬背⼏个知识点。我们的⽬的是建⽴时间复杂度、空间 复杂度意识，写出⾼质量的代码，能够设计基础架构，提升编程技能，训练逻辑思维，积攒⼈⽣经 验，以此获得⼯作回报，实现你的价值，完善你的⼈⽣。

所以，不管你是业务开发⼯程师，还是基础架构⼯程师；不管你是初⼊职场的初级⼯程师，还是⼯ 作多年的资深架构师，⼜或者是想转⼈⼯智能、区块链这些热⻔领域的程序员，数据结构与算法作 为计算机的基础知识、核⼼知识，都是必须要掌握的。

***掌握了数据结构与算法，你看待问题的深度，解决问题的⻆度就会完全不⼀样。***因为这样的你，就 像是站在巨⼈的肩膀上，拿着⽣存利器⾏⾛世界。数据结构与算法，会为你的编程之路，甚⾄⼈⽣ 之路打开⼀扇通往新世界的⼤⻔。

## 02|如何抓住重点，系统⾼效地学习数据结构与算法?

你是否曾跟我⼀样，因为看不懂数据结构和算法，⽽⼀度怀疑是⾃⼰太笨？实际上，很多⼈在第⼀ 次接触这⻔课时，都会有这种感觉，觉得数据结构和算法很抽象，晦涩难懂，宛如天书。正是这个 原因，让很多初学者对这⻔课望⽽却步。

我个⼈觉得，其实真正的原因是你没有找到***好的学习⽅法，没有抓住学习的重点。***实际上，数据结 构和算法的东⻄并不多，常⽤的、基础的知识点更是屈指可数。只要掌握了正确的学习⽅法，学起 来并没有看上去那么难，更不需要什么⾼智商、厚底⼦。

还记得⼤学⾥每次考前⽼师都要划重点吗？今天，我就给你划划我们这⻔课的重点，再告诉你⼀些 我总结的学习⼩窍⻔。相信有了这些之后，你学起来就会有的放⽮、事半功倍了。

### 什么是数据结构？什么是算法？

⼤部分数据结构和算法教材，在开篇都会给这两个概念下⼀个明确的定义。但是，这些定义都很抽 象，对理解这两个概念并没有实质性的帮助，反倒会让你陷⼊死抠定义的误区。毕竟，我们现在学 习，并不是为了考试，所以，概念背得再牢，不会⽤也就没什么⽤。

***虽然我们说没必要深挖严格的定义，但是这并不等于不需要理解概念。***

下⾯我就从⼴义和狭义两个 层⾯，来帮你理解数据结构与算法这两个概念。

从⼴义上讲，数据结构就是指⼀组数据的存储结构。算法就是操作数据的⼀组⽅法。

图书馆储藏书籍你肯定⻅过吧？为了⽅便查找，图书管理员⼀般会将书籍分⻔别类进⾏“存储”。按 照⼀定规律编号，就是书籍这种“数据”的存储结构。

那我们如何来查找⼀本书呢？有很多种办法，你当然可以⼀本⼀本地找，也可以先根据书籍类别的 编号，是⼈⽂，还是科学、计算机，来定位书架，然后再依次查找。笼统地说，这些查找⽅法都是 算法。

从狭义上讲，也就是我们专栏要讲的，是指某些著名的数据结构和算法，⽐如队列、栈、堆、⼆分 查找、动态规划等。这些都是前⼈智慧的结晶，我们可以直接拿来⽤。我们要讲的这些经典数据结 构和算法，都是前⼈从很多实际操作场景中抽象出来的，经过⾮常多的求证和检验，可以⾼效地帮 助我们解决很多实际的开发问题。

那数据结构和算法有什么关系呢？为什么⼤部分书都把这两个东⻄放到⼀块⼉来讲呢？

这是因为，***数据结构和算法是相辅相成的。数据结构是为算法服务的，算法要作⽤在特定的数据结 构之上***。 因此，我们⽆法孤⽴数据结构来讲算法，也⽆法孤⽴算法来讲数据结构。

⽐如，因为数组具有随机访问的特点，常⽤的⼆分查找算法需要⽤数组来存储数据。但如果我们选 择链表这种数据结构，⼆分查找算法就⽆法⼯作了，因为链表并不⽀持随机访问。

数据结构是静态的，它只是组织数据的⼀种⽅式。如果不在它的基础上操作、构建算法，孤⽴存在 的数据结构就是没⽤的。

现在你对数据结构与算法是不是有了⽐较清晰的理解了呢？有了这些储备，下⾯我们来看看，究竟 该怎么学数据结构与算法。 

### 学习这个专栏需要什么基础？

看到数据结构和算法⾥的“算法”两个字，很多⼈就会联想到“数学”，觉得算法会涉及到很多深奥的数 学知识。那我数学基础不是很好，学起来会不会很吃⼒啊？

数据结构和算法课程确实会涉及⼀些数学⽅⾯的推理、证明，尤其是在分析某个算法的时间、空间 复杂度的时候，但是这个你完全不需要担⼼。

这个专栏不会像《算法导论》那样，⾥⾯有⾮常复杂的数学证明和推理。我会由浅⼊深，从概念到 应⽤，⼀点⼀点给你解释清楚。你只要有⾼中数学⽔平，就完全可以学习。

当然，我希望你最好有些编程基础，如果有项⽬经验就更好了。这样我给你讲数据结构和算法如何 提⾼效率、如何节省存储空间，你就会有很直观的感受。因为，对于每个概念和实现过程，我都会 从实际场景出发，不仅教你“是什么”，还会教你“为什么”，并且告诉你遇到同类型问题应该“怎么 做”。 

### 学习的重点在什么地⽅？

提到数据结构和算法，很多⼈就很头疼，因为这⾥⾯的内容实在是太多了。这⾥，我就帮你梳理⼀ 下，应该先学什么，后学什么。你可以对照看看，你属于哪个阶段，然后有针对地进⾏学习。

想要学习数据结构与算法，***⾸先要掌握⼀个数据结构与算法中最重要的概念——复杂度分析。***

这个概念究竟有多重要呢？可以这么说，它⼏乎占了数据结构和算法这⻔课的半壁江⼭，是数据结 构和算法学习的精髓。

数据结构和算法解决的是如何更省、更快地存储和处理数据的问题，因此，我们就需要⼀个考量效 率和资源消耗的⽅法，这就是复杂度分析⽅法。所以，如果你只掌握了数据结构和算法的特点、⽤ 法，但是没有学会复杂度分析，那就相当于只知道操作⼝诀，⽽没掌握⼼法。只有把⼼法了然于 胸，才能做到⽆招胜有招！

所以，复杂度分析这个内容，我会⽤很⼤篇幅给你讲透。你也⼀定要花⼤⼒⽓来啃，必须要拿下， 并且要搞得⾮常熟练。否则，后⾯的数据结构和算法也很难学好。

搞定复杂度分析，下⾯就要进⼊数据结构与算法的正⽂内容了。

为了让你对数据结构和算法能有个全⾯的认识，我画了⼀张图，⾥⾯⼏乎涵盖了所有数据结构和算 法书籍中都会讲到的知识点

但是，作为初学者，或者⼀个⾮算法⼯程师来说，你并不需要掌握图⾥⾯的所有知识点。很多⾼级 的数据结构与算法，⽐如⼆分图、最⼤流等，这些在我们平常的开发中很少会⽤到。所以，你暂时 可以不⽤看。我还是那句话，咱们学习要学会找重点。如果不分重点地学习，眉⽑胡⼦⼀把抓，学 起来肯定会⽐较吃⼒。

所以，结合我⾃⼰的学习⼼得，还有这些年的⾯试、开发经验，我总结了 ***20个最常⽤的、最基础数 据结构与算法，不管是应付⾯试还是⼯作需要，只要集中精⼒逐⼀攻克这 20个知识点就⾜够了***。 这⾥⾯有10个数据结构：数组、链表、栈、队列、散列表、⼆叉树、堆、跳表、图、Trie树；10个 算法：递归、排序、⼆分查找、搜索、哈希算法、贪⼼算法、分治算法、回溯算法、动态规划、字 符串匹配算法。

掌握了这些基础的数据结构和算法，再学更加复杂的数据结构和算法，就会⾮常容易、⾮常快。 在学习数据结构和算法的过程中，你也要注意，不要只是死记硬背，不要为了学习⽽学习，***⽽是要 学习它的“来历”“⾃身的特点”“适合解决的问题”以及“实际的应⽤场景”。***对于每⼀种数据结构或算 法，我都会从这⼏个⽅⾯进⾏详细讲解。只要你掌握了我每节课⾥讲的内容，就能在开发中灵活应⽤。

学习数据结构和算法的过程，是⾮常好的思维训练的过程，所以，千万不要被动地记忆，要多辩证 地思考，多问为什么。如果你⼀直这么坚持做，你会发现，等你学完之后，写代码的时候就会不由 ⾃主地考虑到很多性能⽅⾯的事情，时间复杂度、空间复杂度⾮常⾼的垃圾代码出现的次数就会越 来越少。你的编程内功就真正得到了修炼。

### ⼀些可以让你事半功倍的学习技巧

前⾯我给你划了学习的重点，也讲了学习这⻔课需要具备的基础。作为⼀个过来⼈，现在我就给你 分享⼀下，专栏学习的⼀些技巧。掌握了这些技巧，可以让你化被动为主动，学起来更加轻松，更 加有动⼒！

#### 1.边学边练，适度刷题

“边学边练”这⼀招⾮常有⽤。建议你每周花1～2个⼩时的时间，集中把这周的三节内容涉及的数据 结构和算法，全都⾃⼰写出来，⽤代码实现⼀遍。这样⼀定会⽐单纯地看或者听的效果要好很多！ 有⾯试需求的同学，可能会问了，那我还要不要去刷题呢？

我个⼈的观点是可以***“适度”刷题，但⼀定不要浪费太多时间在刷题上。我们学习的⽬的还是掌握， 然后应⽤。***除⾮你要⾯试Google、Facebook这样的公司，它们的算法题⽬⾮常⾮常难，必须⼤量 刷题，才能在短期内提升应试正确率。如果是应对国内公司的技术⾯试，即便是BAT这样的公司， 你只要彻底掌握这个专栏的内容，就⾜以应对。

#### 2.多问、多思考、多互动

***学习最好的⽅法是，找到⼏个⼈⼀起学习，⼀块⼉讨论切磋，有问题及时寻求⽼师答疑。***但是，离 开⼤学之后，既没有同学也没有⽼师，这个条件就⽐较难具备了。

不过，这也就是咱们专栏学习的优势。专栏⾥有很多跟你⼀样的学习者。你可以多在留⾔区写下⾃ ⼰的疑问、思考和总结，也可以经常看看别⼈的留⾔，和他们进⾏互动。

除此之外，如果你有疑问，你可以随时在留⾔区给我留⾔，我只要有空就会及时回复你。你不要担 ⼼问的问题太⼩⽩。因为我初学的时候，也常常会被⼀些⼩⽩问题困扰。不懂⼀点都不丢⼈，只要 你勇敢提出来，我们⼀起解决了就可以了。

我也会⼒争每节课都最⼤限度地给你讲透，帮你扫除知识盲点，⽽你要做的就是，避免⼀知半解， 要想尽⼀切办法去搞懂我讲的所有内容。

#### 3.打怪升级学习法

***学习的过程中，我们碰到最⼤的问题就是，坚持不下来。***是的，很多基础课程学起来都⾮常枯燥。 为此，我⾃⼰总结了⼀套“打怪升级学习法”。

游戏你肯定玩过吧？为什么很多看起来⾮常简单⼜没有乐趣的游戏，你会玩得不亦乐乎呢？这是因 为，当你努⼒打到⼀定级别之后，每天看着⾃⼰的经验值、战⽃⼒在慢慢提⾼，那种每天都在⼀点 ⼀点成⻓的成就感就不由⾃主地产⽣了

所以，***我们在枯燥的学习过程中，也可以给⾃⼰设⽴⼀个切实可⾏的⽬标，***就像打怪升级⼀样。 ⽐如，针对这个专栏，你就可以设⽴这样⼀个⽬标：每节课后的思考题都认真思考，并且回复到留 ⾔区。当你看到很多⼈给你点赞之后，你就会为了每次都能发⼀个漂亮的留⾔，⽽更加认真地学 习。

当然，还有很多其他的⽬标，⽐如，每节课后都写⼀篇学习笔记或者学习⼼得；或者你还可以每节 课都找⼀下我讲得不对、不合理的地⽅……诸如此类，你可以总结⼀个适合你的“打怪升级攻略”。 如果你能这样学习⼀段时间，不仅能收获到知识，你还会有意想不到的成就感。因为，这其实帮你 改掉了⼀点学习的坏习惯。这个习惯⼀旦改掉了，你的⼈⽣也会变得不⼀样。

#### 4.知识需要沉淀，不要想试图⼀下⼦掌握所有

在学习的过程中，⼀定会碰到“拦路⻁”。如果哪个知识点没有怎么学懂，不要着急，这是正常的。 因为，想听⼀遍、看⼀遍就把所有知识掌握，这肯定是不可能的。***学习知识的过程是反复迭代、不 断沉淀的过程。***

如果碰到“拦路⻁”，你可以尽情地在留⾔区问我，也可以先沉淀⼀下，过⼏天再重新学⼀遍。所 谓，书读百遍其义⾃⻅，我觉得是很有道理的！

我讲的这些学习⽅法，不仅仅针对咱们这⼀个课程的学习，其实完全适⽤任何知识的学习过程。你 可以通过这个专栏的学习，实践⼀下这些⽅法。如果效果不错，再推⼴到之后的学习过程中。

### 内容⼩结

今天，我带你划了划数据结构和算法的学习重点，复杂度分析，以及10个数据结构和10个算法。 这些内容是我根据平时的学习和⼯作、⾯试经验积累，精⼼筛选出来的。只要掌握这些内容，应付 ⽇常的⾯试、⼯作，基本不会有问题。

除此之外，我还给你分享了我总结的⼀些学习技巧，⽐如边学边练、多问、多思考，还有两个⽐较 通⽤的学习⽅法，打怪升级法和沉淀法。掌握了这些学习技巧，可以让你学习过程中事半功倍。所 以，你⼀定要好好实践哦！

### 课后思考

今天的内容是⼀个准备课，从下节开始，我们就要正式开始学习精⼼筛选出的这20个数据结构和算 法了。所以，今天给你布置⼀个任务，对照我上⾯讲的“打怪升级学习法”，请思考⼀下你⾃⼰学习 这个专栏的⽅法，让我们⼀起在留⾔区⽴下Flag，相互⿎励！

另外，你在之前学习数据结构和算法的过程中，遇到过什么样的困难或者疑惑吗？ 欢迎留⾔和我分享，我会第⼀时间给你反馈

## 03|复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗?

我们都知道，数据结构和算法本身解决的是“快”和“省”的问题，即如何让代码运⾏得更快，如何让代 码更省存储空间。所以，执⾏效率是算法⼀个⾮常重要的考量指标。那如何来衡量你编写的算法代 码的执⾏效率呢？这⾥就要⽤到我们今天要讲的内容：时间、空间复杂度分析。

其实，只要讲到数据结构与算法，就⼀定离不开时间、空间复杂度分析。⽽且，我个⼈认为，***复杂 度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了⼀半。***

复杂度分析实在太重要了，因此我准备⽤两节内容来讲。希望你学完这个内容之后，⽆论在任何场 景下，⾯对任何代码的复杂度分析，你都能做到“庖丁解⽜”般游刃有余。 

#### 为什么需要复杂度分析？

你可能会有些疑惑，我把代码跑⼀遍，通过统计、监控，就能得到算法执⾏的时间和占⽤的内存⼤ ⼩。为什么还要做时间、空间复杂度分析呢？这种分析⽅法能⽐我实实在在跑⼀遍得到的数据更准 确吗？

⾸先，我可以肯定地说，你这种评估算法执⾏效率的⽅法是正确的。很多数据结构和算法书籍还给 这种⽅法起了⼀个名字，叫**事后统计法**。但是，这种统计⽅法有⾮常⼤的局限性。

1. 测试结果⾮常依赖测试环境：测试环境中硬件的不同会对测试结果有很⼤的影响。⽐如，我们拿同样⼀段代码，分别⽤Intel Core i9处理器和Intel Core i3处理器来运⾏，不⽤说，i9处理器要⽐i3处理器执⾏的速度快很多。还有， ⽐如原本在这台机器上a代码执⾏的速度⽐b代码要快，等我们换到另⼀台机器上时，可能会有截然 相反的结果。
2. 测试结果受数据规模的影响很⼤：后⾯我们会讲排序算法，我们先拿它举个例⼦。对同⼀个排序算法，待排序数据的有序度不⼀样， 排序的执⾏时间就会有很⼤的差别。极端情况下，如果数据已经是有序的，那排序算法不需要做任 何操作，执⾏时间就会⾮常短。除此之外，如果测试数据规模太⼩，测试结果可能⽆法真实地反应 算法的性能。⽐如，对于⼩规模的数据排序，插⼊排序可能反倒会⽐快速排序要快！所以，***我们需要⼀个不⽤具体的测试数据来测试，就可以粗略地估计算法的执⾏效率的⽅法***。这就 是我们今天要讲的时间、空间复杂度分析⽅法。

#### ⼤O复杂度表示法

算法的执⾏效率，粗略地讲，就是算法代码执⾏的时间。但是，如何在不运⾏代码的情况下，⽤“⾁ 眼”得到⼀段代码的执⾏时间呢？

这⾥有段⾮常简单的代码，求1,2,3…n的累加和。现在，我就带你⼀块来估算⼀下这段代码的执⾏ 时间。

```java
int cal(int n) {
  int sum = 0;
  int i = 1;
  for (; i <= n; ++i) {
  sum = sum + i;
  }
  return sum;
}
```

从CPU的⻆度来看，这段代码的每⼀⾏都执⾏着类似的操作：***读数据-运算-写数据***。

尽管每⾏代码 对应的CPU执⾏的个数、执⾏的时间都不⼀样，但是，我们这⾥只是粗略估计，所以可以假设每⾏ 代码执⾏的时间都⼀样，为`unit_time`。在这个假设的基础之上，这段代码的总执⾏时间是多少呢？ 第2、3⾏代码分别需要1个`unit_time`的执⾏时间，第4、5⾏都运⾏了n遍，所以需要``2n*unit_time`的 执⾏时间，所以这段代码总的执⾏时间就是`(2n+2)*unit_time`。可以看出来，***所有代码的执⾏时间 `T(n)`与每⾏代码的执行次数成正⽐。***

按照这个分析思路，我们再来看这段代码。

```java
int cal(int n) {
  int sum = 0;
  int i = 1;
  int j = 1;
  for (; i <= n; ++i) {
    j = 1;
    for (; j <= n; ++j) {
	  sum = sum + i * j;
	}
  }
}

```

我们依旧假设每个语句的执⾏时间是`unit_time`。那这段代码的总执⾏时间`T(n)`是多少呢？ 第2、3、4⾏代码，每⾏都需要1个`unit_time`的执⾏时间，第5、6⾏代码循环执⾏了n遍，需要`2n * unit_time`的执⾏时间，第7、8⾏代码循环执⾏了`n^2`遍，所以需要`2n * unit_time`的执⾏时间。所 以，整段代码总的执⾏时间`T(n) = (2n^2 + 2n+3)*unit_time`。

尽管我们不知道`unit_time`的具体值，但是通过这两段代码执⾏时间的推导过程，我们可以得到⼀个 ⾮常重要的规律，那就是，***所有代码的执⾏时间`T(n)`与每⾏代码的执⾏次数 n 成正⽐。***

我们可以把这个规律总结成⼀个公式。注意，⼤O就要登场了！
$$
T(n) = O(f(n))
$$
我来具体解释⼀下这个公式。其中，`T(n)`我们已经讲过了，它表示代码执⾏的时间；n 表示数据规模 的⼤⼩；`f(n)`表示每⾏代码执⾏的次数总和。因为这是⼀个公式，所以⽤`f(n)`来表示。公式中的`O`， 表示代码的执⾏时间`T(n)`与`f(n)`表达式成正⽐。

所以，第⼀个例⼦中的`T(n) = O(2n+2)`，第⼆个例⼦中的`T(n) = O(2n +2n+3)`。这就是⼤**`O`时间复杂 度表示法**。⼤`O`时间复杂度实际上并不具体表示代码真正的执⾏时间，⽽是表示***代码执⾏时间随数据规模增⻓的变化趋势***，所以，也叫作**渐进时间复杂度**（asymptotic time complexity），简称**时间复杂度**。

当n很⼤时，你可以把它想象成10000、100000。⽽公式中的低阶、常量、系数三部分并不左右增 ⻓趋势，所以都可以忽略。我们只需要记录⼀个最⼤量级就可以了，如果⽤⼤`O`表示法表示刚讲的 那两段代码的时间复杂度，就可以记为：`T(n) = O(n)`； `T(n) = O(n^2)`。

#### 时间复杂度分析

前⾯介绍了⼤O时间复杂度的由来和表示⽅法。现在我们来看下，如何分析⼀段代码的时间复杂 度？我这⼉有三个⽐较实⽤的⽅法可以分享给你。

##### 只关注循环执⾏次数最多的⼀段代码

我刚才说了，⼤`O`这种复杂度表示⽅法只是表示⼀种变化趋势。我们通常会忽略掉公式中的常量、 低阶、系数，只需要记录⼀个最⼤阶的量级就可以了。所以，我们在分析⼀个算法、⼀段代码的时 间复杂度的时候，也只关注循环执⾏次数最多的那⼀段代码就可以了。这段核⼼代码执⾏次数的 n 的量级，就是整段要分析代码的时间复杂度。

为了便于你理解，我还拿前⾯的例⼦来说明。

```java
int cal(int n) {
  int sum = 0;
  int i = 1;
  for (; i <= n; ++i) {
  sum = sum + i;
  }
  return sum;
}
```

其中第2、3⾏代码都是常量级的执⾏时间，与n的⼤⼩⽆关，所以对于复杂度并没有影响。循环执 ⾏次数最多的是第4、5⾏代码，所以这块代码要重点分析。前⾯我们也讲过，这两⾏代码被执⾏了 n 次，所以总的时间复杂度就是`O(n)`。

##### 加法法则：总复杂度等于量级最⼤的那段代码的复杂度

我这⾥还有⼀段代码。你可以先试着分析⼀下，然后再往下看跟我的分析思路是否⼀样。

```java
int cal(int n) {
  int sum_1 = 0;
  int p = 1;
  for (; p < 100; ++p) {
	sum_1 = sum_1 + p;
  }
  int sum_2 = 0;
  int q = 1;
  for (; q < n; ++q) {
	sum_2 = sum_2 + q;
  }
  int sum_3 = 0;
  int i = 1;
  int j = 1;
  for (; i <= n; ++i) {
    j = 1;
	for (; j <= n; ++j) {
	  sum_3 = sum_3 + i * j;
	}
  }
  return sum_1 + sum_2 + sum_3;
}
```

这⾥我要再强调⼀下，即便这段代码循环10000次、100000次，只要是⼀个已知的数，跟 n ⽆关， 照样也是常量级的执⾏时间。当n⽆限⼤的时候，就可以忽略。尽管对代码的执⾏时间会有很⼤影 响，但是回到时间复杂度的概念来说，它表示的是⼀个算法执⾏效率与数据规模增⻓的变化趋势， 所以不管常量的执⾏时间多⼤，我们都可以忽略掉。因为它本身对增⻓趋势并没有影响。 那第⼆段代码和第三段代码的时间复杂度是多少呢？答案是`O(n)`和`O(n^2)`，你应该能容易就分析出 来，我就不啰嗦了。

综合这三段代码的时间复杂度，我们取其中最⼤的量级。所以，整段代码的时间复杂度就为`O(n^2)`。 也就是说：总的时间复杂度就等于量级最⼤的那段代码的时间复杂度。那我们将这个规律抽象成公 式就是：

如果`T1(n) = O(f(n))`，`T2(n) = O(g(n))`；那么`T(n) = T1(n)+T2(n) = max(O(f(n))`, `O(g(n))) = O(max(f(n), g(n)))`。

##### 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积

我刚讲了⼀个复杂度分析中的加法法则，这⼉还有⼀个乘法法则。类⽐⼀下，你应该能“猜到”公式 是什么样⼦的吧？

如果`T1(n)=O(f(n))`，`T2(n)=O(g(n))`；那么`T(n) = T1(n)*T2(n) = O(f(n))*O(g(n)) = O(f(n)*g(n))`

也就是说，假设`T1(n) = O(n)`，`T2(n) = O(n^2)`，`则T1(n) * T2(n) = O(n^2)`。落实到具体的代码上，我 们可以把乘法法则看成是嵌套循环，我举个例⼦给你解释⼀下。

```java
int cal(int n) {
  int ret = 0;
  int i = 1;
  for (; i < n; ++i) {
	ret = ret + f(i);
  }
}

int f(int n) {
  int sum = 0;
  int i = 1;
  for (; i < n; ++i) {
	sum = sum + i;
  }
  return sum;
}
```

我们单独看cal()函数。假设f()只是⼀个普通的操作，那第4～6⾏的时间复杂度就是，`T1(n) = O(n)`。但`f()`函数本身不是⼀个简单的操作，它的时间复杂度是`T2(n) = O(n)`，所以，整个`cal()`函数的 时间复杂度就是，`T(n) = T1(n) * T2(n) = O(n*n) = O(n^2)`。

我刚刚讲了三种复杂度的分析技巧。不过，你并不⽤刻意去记忆。实际上，复杂度分析这个东⻄关 键在于“熟练”。你只要多看案例，多分析，就能做到“⽆招胜有招”。

#### ⼏种常⻅时间复杂度实例分析

虽然代码千差万别，但是常⻅的复杂度量级并不多。我稍微总结了⼀下，这些复杂度量级⼏乎涵盖 了你今后可以接触的所有代码的复杂度量级。

![](C:\Users\21133\Pictures\js\changjianshijianfuzadu.png)

对于刚罗列的复杂度量级，我们可以粗略地分为两类，**多项式量级**和**⾮多项式量级**。其中，⾮多项式量级只有两个：`O(2^n)`和`O(n!)`。

当数据规模n越来越⼤时，⾮多项式量级算法的执⾏时间会急剧增加，求解问题的执⾏时间会⽆限 增⻓。所以，⾮多项式时间复杂度的算法其实是⾮常低效的算法。因此，关于NP 时间复杂度我就不 展开讲了。我们主要来看⼏种常⻅的多项式时间复杂度。

##### O(1)

⾸先你必须明确⼀个概念，`O(1)`只是常量级时间复杂度的⼀种表示⽅法，并不是指只执⾏了⼀⾏代 码。⽐如这段代码，即便有3⾏，它的时间复杂度也是`O(1）`，⽽不是`O(3)`。

```java
int i = 8;
int j = 6;
int sum = i + j;
```

我稍微总结⼀下，只要代码的执⾏时间不随 n 的增⼤⽽增⻓，这样代码的时间复杂度我们都记作` O(1)`。或者说，⼀般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万⾏的代码， 其时间复杂度也是`Ο(1)`。

##### O(logn)、O(nlogn)

对数阶时间复杂度⾮常常⻅，同时也是最难分析的⼀种时间复杂度。我通过⼀个例⼦来说明⼀下。

```java
i=1;
while (i <= n) {
  i = i * 2;
}
```

根据我们前⾯讲的复杂度分析⽅法，第三⾏代码是循环执⾏次数最多的。所以，我们只要能计算出 这⾏代码被执⾏了多少次，就能知道整段代码的时间复杂度。

从代码中可以看出，变量i的值从1开始取，每循环⼀次就乘以2。当⼤于n时，循环结束。还记得我 们⾼中学过的等⽐数列吗？实际上，变量i的取值就是⼀个等⽐数列。如果我把它⼀个⼀个列出来， 就应该是这个样⼦的：

![](C:\Users\21133\Pictures\js\logshijianfuzadu.png)

所以，我们只要知道x值是多少，就知道这⾏代码执⾏的次数了。通过`2 = n`求解x这个问题我们想高中应该就学过了，我就不多说了。`x=log2 n`，所以，这段代码的时间复杂度就是`O(log2 n)`。 现在，我把代码稍微改下，你再看看，这段代码的时间复杂度是多少？

```java
i=1;
while (i <= n) {
  i = i * 3;
}
```

根据我刚刚讲的思路，很简单就能看出来，这段代码的时间复杂度为`O(log3 n)`。 实际上，不管是以2为底、以3为底，还是以10为底，我们可以把所有对数阶的时间复杂度都记为` O(logn)`。为什么呢？

我们知道，对数之间是可以互相转换的，`log3 n`就等于`log3 2 * log2 n`，所以`O(log3 n) = O(C * log2 n)`， 其中`C=log3 2`是⼀个常量。基于我们前⾯的⼀个理论：在采⽤⼤`O`标记复杂度的时候，可以忽略系 数，即`O(Cf(n)) = O(f(n))`。所以，`O(log3 n)` 就等于`O(log2 n)`。因此，在对数阶时间复杂度的表示⽅ 法⾥，我们忽略对数的“底”，统⼀表示为`O(logn)`。

如果你理解了我前⾯讲的`O(logn)`，那`O(nlogn)`就很容易理解了。还记得我们刚讲的乘法法则吗？如 果⼀段代码的时间复杂度是`O(logn)`，我们循环执⾏ n 遍，时间复杂度就是`O(nlogn)`了。⽽ 且，`O(nlogn)`也是⼀种⾮常常⻅的算法时间复杂度。⽐如，归并排序、快速排序的时间复杂度都是`O(nlogn)`。

##### O(m+n)、O(m*n)

我们再来讲⼀种跟前⾯都不⼀样的时间复杂度，代码的复杂度由**两个数据的规模**来决定。⽼规矩， 先看代码！

```java
int cal(int m, int n) {
  int sum_1 = 0;
  int i = 1;
  for (; i < m; ++i) {
	sum_1 = sum_1 + i;
  }
  int sum_2 = 0;
  int j = 1;
  for (; j < n; ++j) {
	sum_2 = sum_2 + j;
  }
  return sum_1 + sum_2;
}

```

从代码中可以看出，m 和 n 是表示两个数据规模。我们⽆法事先评估 m 和 n 谁的量级⼤，所以我们在 表示复杂度的时候，就不能简单地利⽤加法法则，省略掉其中⼀个。所以，上⾯代码的时间复杂度 就是`O(m+n)`。

针对这种情况，原来的加法法则就不正确了，我们需要将加法规则改为：`T1(m) + T2(n) = O(f(m) + g(n))`。但是乘法法则继续有效：`T1(m)*T2(n) = O(f(m) * f(n))`。

### 空间复杂度分析

前⾯，咱们花了很⻓时间讲⼤O表示法和时间复杂度分析，理解了前⾯讲的内容，空间复杂度分析 ⽅法学起来就⾮常简单了。

前⾯我讲过，时间复杂度的全称是**渐进时间复杂度**，***表示算法的执⾏时间与数据规模之间的增⻓关系***。类⽐⼀下，空间复杂度全称就是**渐进空间复杂度**（asymptotic space complexity），***表示算法的存储空间与数据规模之间的增⻓关系。***

我还是拿具体的例⼦来给你说明。（这段代码有点“傻”，⼀般没⼈会这么写，我这么写只是为了⽅ 便给你解释。）

```java
void print(int n) {
  int i = 0;
  int[] a = new int[n];
  for (i; i <n; ++i) {
	a[i] = i * i;
  }
  for (i = n-1; i >= 0; --i) {
  	print out a[i]
  }
}
```

跟时间复杂度分析⼀样，我们可以看到，第2⾏代码中，我们申请了⼀个空间存储变量`i`，但是它是 常量阶的，跟数据规模 n 没有关系，所以我们可以忽略。第3⾏申请了⼀个⼤⼩为 n 的`int`类型数组， 除此之外，剩下的代码都没有占⽤更多的空间，所以整段代码的空间复杂度就是`O(n)`。

我们常⻅的空间复杂度就是`O(1)`、`O(n)`、`O(n^2)`，像`O(logn)`、`O(nlogn)`这样的对数阶复杂度平时都⽤不到。⽽且，空间复杂度分析⽐时间复杂度分析要简单很多。所以，对于空间复杂度，掌握刚我 说的这些内容已经⾜够了。

### 内容⼩结

基础复杂度分析的知识到此就讲完了，我们来总结⼀下。 复杂度也叫渐进复杂度，包括时间复杂度和空间复杂度，⽤来分析算法执⾏效率与数据规模之间的 增⻓关系，可以粗略地表示，越⾼阶复杂度的算法，执⾏效率越低。常⻅的复杂度并不多，从低阶 到⾼阶有：`O(1)`、`O(logn)`、`O(n)`、`O(nlogn)`、`O(n^2)`。等你学完整个专栏之后，你就会发现⼏乎所 有的数据结构和算法的复杂度都跑不出这⼏个。



![](C:\Users\21133\Pictures\js\fuzaduquxian.png)

复杂度分析并不难，关键在于多练。 之后讲后⾯的内容时，我还会带你详细地分析每⼀种数据结构 和算法的时间、空间复杂度。只要跟着我的思路学习、练习，你很快就能和我⼀样，每次看到代码 的时候，简单的⼀眼就能看出其复杂度，难的稍微分析⼀下就能得出答案。 

### 课后思考

有⼈说，我们项⽬之前都会进⾏性能测试，再做代码的时间复杂度、空间复杂度分析，是不是多此 ⼀举呢？⽽且，每段代码都分析⼀下时间复杂度、空间复杂度，是不是很浪费时间呢？你怎么看待 这个问题呢？

## 041复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度

上⼀节，我们讲了复杂度的⼤`O`表示法和⼏个分析技巧，还举了⼀些常⻅复杂度分析的例⼦，⽐如` O(1)`、`O(logn)`、`O(n)`、`O(nlogn)`复杂度分析。掌握了这些内容，对于复杂度分析这个知识点，你已 经可以到及格线了。但是，我想你肯定不会满⾜于此。

今天我会继续给你讲四个复杂度分析⽅⾯的知识点，最好情况时间复杂度（`best case time complexity`）、最坏情况时间复杂度（`worst case time complexity`）、平均情况时间复杂 度（`average case time complexity`）、均摊时间复杂度（`amortized time complexity`）。如果这⼏ 个概念你都能掌握，那对你来说，复杂度分析这部分内容就没什么⼤问题了。

### 最好、最坏情况时间复杂度

上⼀节我举的分析复杂度的例⼦都很简单，今天我们来看⼀个稍微复杂的。你可以⽤我上节教你的 分析技巧，⾃⼰先试着分析⼀下这段代码的时间复杂度。

```java
// n表示数组的array长度
int find (int[] array, int n, int x) {
  int i = 0;
  int pos = -1;
  for (; i < n ; ++i) {
    if (array[i] == x) pos = i
  }
  return pos
}
```

你应该可以看出来，这段代码要实现的功能是，在⼀个⽆序的数组（`array`）中，查找变量 x 出现的 位置。如果没有找到，就返回-1。按照上节课讲的分析⽅法，这段代码的复杂度是`O(n)`，其中，n代 表数组的⻓度。

我们在数组中查找⼀个数据，并不需要每次都把整个数组都遍历⼀遍，因为有可能中途找到就可以 提前结束循环了。但是，这段代码写得不够⾼效。我们可以这样优化⼀下这段查找代码。

```java
// n表示数组array的⻓度
int find(int[] array, int n, int x) {
  int i = 0;
  int pos = -1;
  for (; i < n; ++i) {
	if (array[i] == x) {
	  pos = i;
	  break;
    }
  }
  return pos;
}

```

这个时候，问题就来了。我们优化完之后，这段代码的时间复杂度还是`O(n)`吗？很显然，咱们上⼀ 节讲的分析⽅法，解决不了这个问题。

因为，要查找的变量 x 可能出现在数组的任意位置。如果数组中第⼀个元素正好是要查找的变量x， 那就不需要继续遍历剩下的`n-1`个数据了，那时间复杂度就是`O(1)`。但如果数组中不存在变量x，那 我们就需要把整个数组都遍历⼀遍，时间复杂度就成了`O(n)`。所以，不同的情况下，这段代码的时 间复杂度是不⼀样的。

为了表示代码在不同情况下的不同时间复杂度，我们需要引⼊三个概念：**最好情况时间复杂度**、**最坏情况时间复杂度**和**平均情况时间复杂度**。

顾名思义，***最好情况时间复杂度就是，在最理想的情况下，执⾏这段代码的时间复杂度***。就像我们刚刚讲到的，在最理想的情况下，要查找的变量x正好是数组的第⼀个元素，这个时候对应的时间复 杂度就是最好情况时间复杂度。

同理，***最坏情况时间复杂度就是，在最糟糕的情况下，执⾏这段代码的时间复杂度***。就像刚举的那 个例⼦，如果数组中没有要查找的变量x，我们需要把整个数组都遍历⼀遍才⾏，所以这种最糟糕情 况下对应的时间复杂度就是最坏情况时间复杂度。

### 平均情况时间复杂度

我们都知道，最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码复杂度，发 ⽣的概率其实并不⼤。为了更好地表示平均情况下的复杂度，我们需要引⼊另⼀个概念：平均情况时间复杂度，后⾯我简称为平均时间复杂度。

平均时间复杂度⼜该怎么分析呢？我还是借助刚才查找变量x的例⼦来给你解释。 要查找的变量 x 在数组中的位置，有`n+1`种情况：在数组的`0～n-1`位置中和不在数组中。我们把每种情况下，查找需要遍历的元素个数累加起来，然后再除以`n+1`，就可以得到需要遍历的元素个数的 平均值，即：

![](C:\Users\21133\Pictures\js\pingjunshijianfuzadu.png)

我们知道，时间复杂度的⼤`O`标记法中，可以省略掉系数、低阶、常量，所以，咱们把刚刚这个公 式简化之后，得到的平均时间复杂度就是`O(n)`。 这个结论虽然是正确的，但是计算过程稍微有点⼉问题。究竟是什么问题呢？我们刚讲的这`n+1`种 情况，出现的概率并不是⼀样的。我带你具体分析⼀下。（这⾥要稍微⽤到⼀点⼉概率论的知识， 不过⾮常简单，你不⽤担⼼。）

我们知道，要查找的变量x，要么在数组⾥，要么就不在数组⾥。这两种情况对应的概率统计起来很 麻烦，为了⽅便你理解，我们假设在数组中与不在数组中的概率都为`1/2`。另外，要查找的数据出现 在0～`n-1`这 n 个位置的概率也是⼀样的，为`1/n`。所以，根据概率乘法法则，要查找的数据出现在`0～ n-1`中任意位置的概率就是`1/(2n)`。

因此，前⾯的推导过程中存在的最⼤问题就是，没有将各种情况发⽣的概率考虑进去。如果我们把 每种情况发⽣的概率也考虑进去，那平均时间复杂度的计算过程就变成了这样：

![](C:\Users\21133\Pictures\js\pingjunfuzadu2.png)

这个值就是概率论中的加权平均值，也叫作期望值，所以平均时间复杂度的全称应该叫**加权平均时间复杂度**或者**期望时间复杂度**。

引⼊概率之后，前⾯那段代码的加权平均值为`(3n+1)/4`。⽤⼤`O`表示法来表示，去掉系数和常量，这 段代码的加权平均时间复杂度仍然是`O(n)`。

你可能会说，平均时间复杂度分析好复杂啊，还要涉及概率论的知识。实际上，在⼤多数情况下， 我们并不需要区分最好、最坏、平均情况时间复杂度三种情况。像我们上⼀节课举的那些例⼦那 样，很多时候，我们使⽤⼀个复杂度就可以满⾜需求了。只有同⼀块代码在不同的情况下，时间复 杂度有量级的差距，我们才会使⽤这三种复杂度表示法来区分。

### 均摊时间复杂度

到此为⽌，你应该已经掌握了算法复杂度分析的⼤部分内容了。下⾯我要给你讲⼀个更加⾼级的概 念，均摊时间复杂度，以及它对应的分析⽅法，摊还分析（或者叫平摊分析）。

均摊时间复杂度，听起来跟平均时间复杂度有点⼉像。对于初学者来说，这两个概念确实⾮常容易 弄混。我前⾯说了，⼤部分情况下，我们并不需要区分最好、最坏、平均三种复杂度。平均复杂度 只在某些特殊情况下才会⽤到，⽽均摊时间复杂度应⽤的场景⽐它更加特殊、更加有限。

⽼规矩，我还是借助⼀个具体的例⼦来帮助你理解。（当然，这个例⼦只是我为了⽅便讲解想出来 的，实际上没⼈会这么写。）

```java
// array表示⼀个⻓度为n的数组
// 代码中的array.length就等于n
int[] array = new int[n];
int count = 0;
void insert(int val) {
  if (count == array.length) {
	int sum = 0;
	for (int i = 0; i < array.length; ++i) {
	  sum = sum + array[i];
	}
	array[0] = sum;
	count = 1;
  }
    
  array[count] = val;
  ++count;
}

```

我先来解释⼀下这段代码。这段代码实现了⼀个往数组中插⼊数据的功能。当数组满了之后，也就 是代码中的`count == array.length`时，我们⽤`for`循环遍历数组求和，并清空数组，将求和之后的 `sum`值放到数组的第⼀个位置，然后再将新的数据插⼊。但如果数组⼀开始就有空闲空间，则直接 将数据插⼊数组。

那这段代码的时间复杂度是多少呢？你可以先⽤我们刚讲到的三种时间复杂度的分析⽅法来分析⼀ 下。

最理想的情况下，数组中有空闲空间，我们只需要将数据插⼊到数组下标为`count`的位置就可以了， 所以最好情况时间复杂度为`O(1)`。最坏的情况下，数组中没有空闲空间了，我们需要先做⼀次数组 的遍历求和，然后再将数据插⼊，所以最坏情况时间复杂度为`O(n)`。

那平均时间复杂度是多少呢？答案是`O(1)`。我们还是可以通过前⾯讲的概率论的⽅法来分析。 假设数组的⻓度是n，根据数据插⼊的位置的不同，我们可以分为n种情况，每种情况的时间复杂度 是`O(1)`。除此之外，还有⼀种“额外”的情况，就是在数组没有空闲空间时插⼊⼀个数据，这个时候的时间复杂度是`O(n)`。⽽且，这`n+1`种情况发⽣的概率⼀样，都是`1/(n+1)`。所以，根据加权平均的 计算⽅法，我们求得的平均时间复杂度就是：

![](C:\Users\21133\Pictures\js\juntanfuzadu1.png)

⾄此为⽌，前⾯的最好、最坏、平均时间复杂度的计算，理解起来应该都没有问题。但是这个例⼦ ⾥的平均复杂度分析其实并不需要这么复杂，不需要引⼊概率论的知识。这是为什么呢？我们先来 对⽐⼀下这个`insert()`的例⼦和前⾯那个`find()`的例⼦，你就会发现这两者有很⼤差别。

⾸先，`find()`函数在极端情况下，复杂度才为`O(1)`。但`insert()`在⼤部分情况下，时间复杂度都为`O(1)`。只有个别情况下，复杂度才⽐较⾼，为`O(n)`。这是`insert()`第⼀个区别于`find()`的地⽅。

我们再来看第⼆个不同的地⽅。对于`insert()`函数来说，`O(1)`时间复杂度的插⼊和`O(n)`时间复杂度的 插⼊，出现的频率是⾮常有规律的，⽽且有⼀定的前后时序关系，⼀般都是⼀个`O(n)`插⼊之后，紧 跟着`n-1`个`O(1)`的插⼊操作，循环往复。

所以，针对这样⼀种特殊场景的复杂度分析，我们并不需要像之前讲平均复杂度分析⽅法那样，找 出所有的输⼊情况及相应的发⽣概率，然后再计算加权平均值。

针对这种特殊的场景，我们引⼊了⼀种更加简单的分析⽅法：**摊还分析法**，通过摊还分析得到的时间复杂度我们起了⼀个名字，**叫均摊时间复杂度**。

那究竟如何使⽤摊还分析法来分析算法的均摊时间复杂度呢？

我们还是继续看在数组中插⼊数据的这个例⼦。每⼀次`O(n)`的插⼊操作，都会跟着`n-1`次`O(1)`的插⼊ 操作，所以把耗时多的那次操作均摊到接下来的`n-1`次耗时少的操作上，均摊下来，这⼀组连续的操作的均摊时间复杂度就是`O(1)`。这就是均摊分析的⼤致思路。你都理解了吗？

均摊时间复杂度和摊还分析应⽤场景⽐较特殊，所以我们并不会经常⽤到。为了⽅便你理解、记 忆，我这⾥简单总结⼀下它们的应⽤场景。如果你遇到了，知道是怎么回事⼉就⾏了。

对⼀个数据结构进⾏⼀组连续操作中，⼤部分情况下时间复杂度都很低，只有个别情况下时间复杂 度⽐较⾼，⽽且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这⼀组操作放在 ⼀块⼉分析，看是否能将较⾼时间复杂度那次操作的耗时，平摊到其他那些时间复杂度⽐较低的操 作上。⽽且，在能够应⽤均摊时间复杂度分析的场合，⼀般均摊时间复杂度就等于最好情况时间复杂度。

尽管很多数据结构和算法书籍都花了很⼤⼒⽓来区分平均时间复杂度和均摊时间复杂度，但其实我 个⼈认为，均摊时间复杂度就是⼀种特殊的平均时间复杂度，我们没必要花太多精⼒去区分它们。 你最应该掌握的是它的分析⽅法，摊还分析。⾄于分析出来的结果是叫平均还是叫均摊，这只是个说法，并不重要。 

### 内容⼩结

今天我们学习了⼏个复杂度分析相关的概念，分别有：最好情况时间复杂度、最坏情况时间复杂 度、平均情况时间复杂度、均摊时间复杂度。之所以引⼊这⼏个复杂度概念，是因为，同⼀段代 码，在不同输⼊的情况下，复杂度量级有可能是不⼀样的

在引⼊这⼏个概念之后，我们可以更加全⾯地表示⼀段代码的执⾏效率。⽽且，这⼏个概念理解起 来都不难。最好、最坏情况下的时间复杂度分析起来⽐较简单，但平均、均摊两个复杂度分析相对 ⽐较复杂。如果你觉得理解得还不是很深⼊，不⽤担⼼，在后续具体的数据结构和算法学习中，我 们可以继续慢慢实践！

### 课后思考

我们今天学的⼏个复杂度分析⽅法，你都掌握了吗？你可以⽤今天学习的知识，来分析⼀下下⾯这 个`add()`函数的时间复杂度。

```java
// 全局变量，⼤⼩为10的数组array，⻓度len，下标i。
int array[] = new int[10];
int len = 10;
int i = 0;
// 往数组中添加⼀个元素
void add(int element) {
  if (i >= len) { // 数组空间不够了
    // 重新申请⼀个2倍⼤⼩的数组空间
    int new_array[] = new int[len*2];
    // 把原来array数组中的数据依次copy到new_array
    for (int j = 0; j < len; ++j) {
      new_array[j] = array[j];
    }
    // new_array复制给array，array现在⼤⼩就是2倍len了
    array = new_array;
    len = 2 * len;
  }
  // 将element放到下标为i的位置，下标i加⼀
  array[i] = element;
  ++i;
}
```

最好情况下`i < len`，只进行一次插入操作，时间复杂度为`O(1)`

最坏情况下`i >= len`，进入判断条件内，进行一层`for`循环，时间复杂度为`O(n)`

平均复杂度：`O(1)`，数组长度为`len`，在`i<len`之前，只需要进行一次插入操作，时间复杂度为`O(1)`，只有当`i`达到`len`之后，才需要进入循环遍历数组，此时时间复杂度为`O(n)`，所有概率相加省去系数等，时间复杂度为`O(1)`。

均摊复杂度：`O(1)`，将最后一次复杂度为`O(n)`的操作均摊到之前`n-1`次复杂度为`O(1)`的操作上，结果为`O(1)`。

## 不定期福利第⼀期|数据结构与算法学习书单

你好，我是王争。欢迎来到不定期更新的周末福利时间。

专栏已经上线两周了，看到这么多⼈在留⾔区写下⾃⼰的疑惑或者观点，我特别开⼼。在留⾔⾥， 很多同学让我推荐⼀些学习数据结构与算法的书籍。因此我特意跟编辑商量了，给你⼀个周末福 利。所以这⼀期呢，我们就来聊⼀聊数据结构和算法学习过程中有哪些必读书籍。

有的同学还在读⼤学，代码还没写过⼏⾏；有的同学已经⼯作数⼗年，这之间的差别还是挺⼤的。 ⽽不同基础的⼈，适宜看的书是完全不⼀样的。因此，针对不同层次、不同语⾔的同学，我分别推 荐了不同的书。希望每个同学，都能找到适合⾃⼰的学习资料，都能在现有⽔平上有所提⾼。 

### 针对⼊⻔的趣味书

⼊⻔的同学，我建议你不要过度追求上去就看经典书。像《算法导论》《算法》这些书，虽然⽐较 经典、⽐较权威，但是⾮常厚。初学就去啃这些书肯定会很费劲。⽽⼀旦啃不下来，挫败感就会很 强。所以，⼊⻔的同学，我建议你找⼀些⽐较容易看的书来看，⽐如《⼤话数据结构》和《算法图 解》。***不要太在意书写得深浅，重要的是能不能坚持看完***。

**《⼤话数据结构》** 这本书最⼤的特点是，它把理论讲得很有趣，不枯燥。⽽且每个数据结构和算 法，作者都结合⽣活中的例⼦进⾏了讲解， 能让你有⾮常直观的感受。虽然这本书有400多⻚，但 是花两天时间读完，应该是没问题的。如果你之前完全不懂数据结构和算法，可以先从这本书看起。

**《算法图解》** 跟《⼤话数据结构》⾛的是同样的路线，就像这本书副标题写的那样，“像⼩说⼀样 有趣的算法⼊⻔书”，主打“图解”，通俗易懂。它只有不到200⻚，所以内容⽐较少。作为⼊⻔，看看这本书，能让你对数据结构和算法有个⼤概的认识。

这些⼊⻔书共同的问题是，缺少细节，不够系统，也不够严谨。所以，如果你想要系统地学数据结 构和算法，看这两本书肯定是不够的。

### 针对特定编程语⾔的教科书

讲数据结构和算法，肯定会跟代码实现挂钩。所以，很多⼈就很关⼼，某某书籍是⽤什么语⾔实现 的，是不是⾃⼰熟悉的语⾔。市⾯⼤部分数据结构和算法书籍都是⽤`C`、`C++`、`Java`语⾔实现的， 还有些是⽤伪代码。⽽使⽤`Python`、`Go`、`PHP`、`JavaScript`、`Objective-C`这些编程语⾔实现的就更 少了。

我这⾥推荐《数据结构和算法分析》。国内外很多⼤学都拿这本书当作教材。这本书⾮常系统、全 ⾯、严谨，⽽且⼜不是特别难，适合对数据结构和算法有些了解，并且掌握了⾄少⼀⻔编程语⾔的 同学。⽽且，这个作者也很⽤⼼。他⽤了三种语⾔，写了三个版本，分别是：**《数据结构与算法分 析 ：C语⾔描述》《数据结构与算法分析：C++描述》《数据结构与算法分析：Java语⾔描述》**。 如果你熟悉的是`Python`或者`JavaScript`，可以参考《数据结构与算法`JavaScript`描述》《数据结构与算法：`Python`语⾔描述》 。⾄于其他语⾔的算法书籍，确实⽐较少。如果你有推荐，可以在留 ⾔区补充⼀下。

### ⾯试必刷的宝典

算法对⾯试很重要，很多⼈也很关⼼。我这⾥推荐⼏本有益于⾯试的书籍，分别是：《剑指offer》 《编程珠玑》《编程之美》。

从**《剑指offer》**这本书的名字就可以看出，作者的写作⽬的⾮常明确，就是为了⾯试。这本书⼏乎 包含了所有常⻅的、经典的⾯试题。如果能搞懂这本书⾥的内容，应付⼀般公司的⾯试应该不成问 题。

**《编程珠玑》**这本书的⾖瓣评分⾮常⾼，有9分。这本书最⼤的特⾊就是讲了很多针对海量数据的 处理技巧。这个可能是其他算法书籍很少涉及的。⾯试的时候，海量数据处理的问题也是经常会问 的，特别是校招⾯试。不管是开拓眼界，还是应付⾯试，这本书都很值得⼀看。

《编程之美》这本书有多位作者，其中绝⼤部分是微软的⼯程师，所以书的质量很有保证。不过， 这⾥⾯的算法题⽬稍微有点难，也不是很系统，这也是我把它归到⾯试这⼀部分的原因。如果你有 ⼀定基础，也喜欢钻研些算法问题，或者要⾯试`Google`、`Facebook`这样的公司，可以拿这本书⾥ 的题，先来⾃测⼀下。

### 经典⼤部头

很多⼈⼀提到算法书就会搬出《算法导论》和《算法》。这两本确实⾮常经典，但是都太厚了，看 起来⽐较费劲，我估计很少有⼈能坚持全部看下来。如果你想更加深⼊地学⼀学数据结构和算法， 我还是强烈建议你看看。

我个⼈觉得，**《算法导论》**这本书的章节安排不是循序渐进的，⾥⾯充斥着各种算法的正确性、复 杂度的证明、推导，数学公式⽐较多，⼀般⼈看起来会⽐较吃⼒。所以，作为⼊⻔书籍，并不是很 推荐。

**《算法》**这本书也是⼀本经典⼤部头，不过它⽐起《算法导论》来要友好很多，更容易看懂，更适 合初学者⼊⻔。但是这本书的缺点也很明显，就是内容不够全⾯，⽐如动态规划这么重要的知识 点，这本书就没有讲。对于数据结构的东⻄，它讲的也不多，基本就是偏重讲算法

### 殿堂级经典

说到殿堂级经典书，如果**《计算机程序设计艺术》**称第⼆，我想没⼈敢称第⼀。这本书包括很多 卷。说实话，我也只看过⽐较简单的⼏卷，⽐如《基本算法》《排序和查找》。

这套书的深度、⼴度、系统性、全⾯性是其他所有数据结构和算法书籍都⽆法相⽐的。但是，如果 你对算法和数据结构不是特别感兴趣，没有很好的数学、算法、计算机基础，想要把这套书读完、 读懂是⽐较难的。你可以把它当作你算法学习的终极挑战。

### 闲暇阅读

算法⽆处不在。我这⾥再推荐⼏本适合闲暇时间阅读的书：**《算法帝国》《数学之美》《算法之 美》**。

这些书共同的特点是，都列举了⼤量的例⼦，⾮常通俗易懂。夸张点说，像《算法帝国》，⽂科⽣ 都能读懂。当你看这些书的时候，你常常会深深感受到算法的⼒量，被算法的优美之处折服。即便 不是从事IT⼯作的，看完这⼏本书也可以开拓眼界。

![](C:\Users\21133\Pictures\js\suanfashutuijian.png)

书籍差不多就是这些。除此之外，留⾔区很多⼈问到算法的实现语⾔。我这⾥也解释⼀下。因为我 现在⽐较常⽤的编程语⾔是`Java`。所以，在专栏⾥，特别简单的、不涉及⾼级语法的，我会⽤`Java `或者`C`、`C++`来实现。稍微复杂的，为了让你能看懂，我会⽤伪代码。所以你完全不⽤担⼼语⾔的 问题。

每节课中有需要代码实现的数据结构和算法，我都另外⽤`Java`语⾔实现⼀遍，然后放到`Github`上， 供你参考。Github的地址我放在这⾥，你可以收藏⼀下：

[GitHub - wangzheng0822/algo: 数据结构和算法必知必会的50个代码实现](https://github.com/wangzheng0822/algo)

⾄于其他语⾔的同学，⽐如`C`、`C++`、`Python`、`Go`、`PHP`、`JavaScript`、`Objective-C`等，我想了⼀ 个`crowd sourcing`的⽅法。

我希望基础较好的同学，参照我的`Java`实现，⽤你熟悉的编程语⾔再实现⼀遍，并且将代码留⾔给 我。如果你写得正确，我会将你的代码上传到`Github`上，分享给更多⼈。

还有⼈问，我学完这个专栏，就可以拿下数据结构和算法吗？我想说的是，***每个⼈的基础、学习能力都不⼀样，掌握程度取决于你的努⼒程度***。除了你之外，没有⼈能百分之百保证你能掌握什么知 识。

有的同学只是把每⼀节课听下来、看下来，就束之⾼阁，也不求甚解，那效果肯定会很差。⽽有些 同学除了听、看之外，遇到不懂的会⾃⼰去查资料、看参考书籍，还会把我讲的数据结构和算法都认真地实现⼀遍，这样的学习效果⾃然就⽐只听⼀遍、看⼀遍要好很多。即便我已经尽我所能我这 些知识讲得深⼊浅出，通俗易懂，但是学习依然还是要靠你⾃⼰啊。

这种答疑的⽅式也会成为我们之后的固定动作，我会把留⾔⾥有价值的问题和反馈沉淀下来，希望 对你的⽇常学习起到补充作⽤。***如果你有什么看不懂、听不懂的地⽅，或者⼯作中有遇到算法问题、技术难题，欢迎写在留⾔区***。（我发现留⾔区⾥卧⻁藏⻰啊，没事⼉可以多扫扫留⾔区。） 这次的周末福利时间就到这啦，我们下次⻅！

## 为什么很多编程语⾔中数组都从0开始编号?

提到数组，我想你肯定不陌⽣，甚⾄还会⾃信地说，它很简单啊。

是的，在每⼀种编程语⾔中，基本都会有数组这种数据类型。不过，它不仅仅是⼀种编程语⾔中的 数据类型，还是⼀种最基础的数据结构。尽管数组看起来⾮常基础、简单，但是我估计很多⼈都并 没有理解这个基础数据结构的精髓。

在⼤部分编程语⾔中，数组都是从0开始编号的，但你是否下意识地想过，**为什么数组要从0开始编 号，⽽不是从1开始呢？** 从1开始不是更符合⼈类的思维习惯吗？

### 如何实现随机访问？

什么是数组？我估计你⼼中已经有了答案。不过，我还是想⽤专业的话来给你做下解释。***数组 （Array）是⼀种线性表数据结构。它⽤⼀组连续的内存空间，来存储⼀组具有相同类型的数据。***这个定义⾥有⼏个关键词，理解了这⼏个关键词，我想你就能彻底掌握数组的概念了。下⾯就从我 的⻆度分别给你“点拨”⼀下。

第⼀是**线性表**（`Linear List`）。顾名思义，线性表就是数据排成像⼀条线⼀样的结构。每个线性表 上的数据最多只有前和后两个⽅向。其实除了数组，链表、队列、栈等也是线性表结构。

![](C:\Users\21133\Pictures\js\xianxingbiao.png)

⽽与它相对⽴的概念是***非线性表***，⽐如⼆叉树、堆、图等。之所以叫⾮线性，是因为，在⾮线性表 中，数据之间并不是简单的前后关系。

![](C:\Users\21133\Pictures\js\非线性表.png)

第⼆个是**连续的内存空间和相同类型的数据**。正是因为这两个限制，它才有了⼀个堪称“杀⼿锏”的 特性：“随机访问”。但有利就有弊，这两个限制也让数组的很多操作变得⾮常低效，⽐如要想在数 组中删除、插⼊⼀个数据，为了保证连续性，就需要做⼤量的数据搬移⼯作。

说到数据的访问，那你知道数组是如何实现根据下标随机访问数组元素的吗？

我们拿⼀个⻓度为10的`int`类型的数组`int[] a = new int[10]`来举例。在我画的这个图中，计算机给数组`a[10]`，分配了⼀块连续内存空间`1000～1039`，其中，内存块的⾸地址为`base_address = 1000`。

![](C:\Users\21133\Pictures\js\数组连续内存.png)

我们知道，计算机会给每个内存单元分配⼀个地址，计算机通过地址来访问内存中的数据。当计算 机需要随机访问数组中的某个元素时，它会⾸先通过下⾯的寻址公式，计算出该元素存储的内存地 址：

```java
a[i]_address = base_address + i * data_type_size
```

其中`data_type_size`表示数组中每个元素的⼤⼩。我们举的这个例⼦⾥，数组中存储的是`int`类型数，所以`data_type_size`就为4个字节。这个公式⾮常简单，我就不多做解释了。

这⾥我要特别纠正⼀个“错误”。我在⾯试的时候，常常会问数组和链表的区别，很多⼈都回答 说，“链表适合插⼊、删除，时间复杂度`O(1)`；数组适合查找，查找时间复杂度为`O(1)`”。

实际上，这种表述是不准确的。数组是适合查找操作，但是查找的时间复杂度并不为`O(1)`。即便是 排好序的数组，你⽤⼆分查找，时间复杂度也是`O(logn)`。所以，正确的表述应该是，***数组⽀持随机 访问，根据下标随机访问的时间复杂度为`O(1)`***。

### 低效的“插⼊”和“删除” 

前⾯概念部分我们提到，数组为了保持内存数据的连续性，会导致插⼊、删除这两个操作⽐较低 效。现在我们就来详细说⼀下，究竟为什么会导致低效？⼜有哪些改进⽅法呢？

我们先来看**插⼊操作**。

假设数组的⻓度为`n`，现在，如果我们需要将⼀个数据插⼊到数组中的第`k`个位置。为了把第`k`个位 置腾出来，给新来的数据，我们需要将第`k～n`这部分的元素都顺序地往后挪⼀位。那插⼊操作的时 间复杂度是多少呢？你可以⾃⼰先试着分析⼀下。

如果在数组的末尾插⼊元素，那就不需要移动数据了，这时的时间复杂度为`O(1)`。但如果在数组的 开头插⼊元素，那所有的数据都需要依次往后移动⼀位，所以最坏时间复杂度是`O(n)`。 因为我们在 每个位置插⼊元素的概率是⼀样的，所以平均情况时间复杂度为`(1+2+…n)/n=O(n)`。

如果数组中的数据是有序的，我们在某个位置插⼊⼀个新的元素时，就必须按照刚才的⽅法搬移k之 后的数据。但是，如果数组中存储的数据并没有任何规律，数组只是被当作⼀个存储数据的集合。 在这种情况下，如果要将某个数组插⼊到第`k`个位置，为了避免⼤规模的数据搬移，我们还有⼀个简单的办法就是，直接将第`k`位的数据搬移到数组元素的最后，把新的元素直接放⼊第k个位置。

为了更好地理解，我们举⼀个例⼦。假设数组`a[10]`中存储了如下5个元素：`a，b，c，d，e`。 我们现在需要将元素`x`插⼊到第3个位置。我们只需要将`c`放⼊到`a[5]`，将`a[2]`赋值为`x`即可。最后，数 组中的元素如下： `a，b，x，d，e，c`。

![](C:\Users\21133\Pictures\js\数组插入简化.png)

利⽤这种处理技巧，在特定场景下，在第`k`个位置插⼊⼀个元素的时间复杂度就会降为`O(1)`。这个处 理思想在快排中也会⽤到，我会在排序那⼀节具体来讲，这⾥就说到这⼉。

我们再来看**删除操作**。

跟插⼊数据类似，如果我们要删除第`k`个位置的数据，为了内存的连续性，也需要搬移数据，不然中 间就会出现空洞，内存就不连续了。

和插⼊类似，如果删除数组末尾的数据，则最好情况时间复杂度为`O(1)`；如果删除开头的数据，则最坏情况时间复杂度为`O(n)`；平均情况时间复杂度也为`O(n)`。

实际上，在某些特殊场景下，我们并不⼀定⾮得追求数组中数据的连续性。如果我们将多次删除操 作集中在⼀起执⾏，删除的效率是不是会提⾼很多呢？

我们继续来看例⼦。数组`a[10]`中存储了8个元素：`a，b，c，d，e，f，g，h`。现在，我们要依次删除`a，b，c`三个元素。

![](C:\Users\21133\Pictures\js\数组删除简化.png)

为了避免`d，e，f，g，h`这⼏个数据会被搬移三次，我们可以先记录下已经删除的数据。每次的删除 操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再 触发执⾏⼀次真正的删除操作，这样就⼤⼤减少了删除操作导致的数据搬移。

如果你了解`JVM`，你会发现，这不就是`JVM`标记清除垃圾回收算法的核⼼思想吗？没错，数据结构 和算法的魅⼒就在于此，***很多时候我们并不是要去死记硬背某个数据结构或者算法，⽽是要学习它 背后的思想和处理技巧，这些东⻄才是最有价值的***。如果你细⼼留意，不管是在软件开发还是架构 设计中，总能找到某些算法和数据结构的影⼦。

### 警惕数组的访问越界问题

了解了数组的⼏个基本操作后，我们来聊聊数组访问越界的问题。

⾸先，我请你来分析⼀下这段`C`语⾔代码的运⾏结果：

```c
int main(int argc, char* argv[]){
  int i = 0;
  int arr[3] = {0};
  for(; i<=3; i++){
    arr[i] = 0;
    printf("hello world\n");
  }
  return 0;
}

```

你发现问题了吗？这段代码的运⾏结果并⾮是打印三⾏“`hello word`”，⽽是会⽆限打印“`hello world`”，这是为什么呢？

因为，数组⼤⼩为3，`a[0]，a[1]，a[2]`，⽽我们的代码因为书写错误，导致`for`循环的结束条件 错写 为了`i<=3`⽽⾮`i<3`，所以当`i=3`时，数组`a[3]`访问越界。

我们知道，在`C`语⾔中，只要不是访问受限的内存，所有的内存空间都是可以⾃由访问的。根据我 们前⾯讲的数组寻址公式，`a[3]`也会被定位到某块不属于数组的内存地址上，⽽这个地址正好是存储变量`i`的内存地址，那么`a[3]=0`就相当于`i=0`，所以就会导致代码⽆限循环。

数组越界在`C`语⾔中是⼀种未决⾏为，并没有规定数组访问越界时编译器应该如何处理。因为，访 问数组的本质就是访问⼀段连续内存，只要数组通过偏移计算得到的内存地址是可⽤的，那么程序 就可能不会报任何错误。

这种情况下，⼀般都会出现莫名其妙的逻辑错误，就像我们刚刚举的那个例⼦，`debug`的难度⾮常 的⼤。⽽且，很多计算机病毒也正是利⽤到了代码中的数组越界可以访问⾮法地址的漏洞，来攻击 系统，所以写代码的时候⼀定要警惕数组越界。

但并⾮所有的语⾔都像`C`⼀样，把数组越界检查的⼯作丢给程序员来做，像Java本身就会做越界检 查，⽐如下⾯这⼏⾏`Java`代码，就会抛出`java.lang.ArrayIndexOutOfBoundsException`。

```java
int[] a = new int[3];
a[3] = 10;
```

### 容器能否完全替代数组？

针对数组类型，很多语⾔都提供了容器类，⽐如`Java`中的`ArrayList`、`C++ STL`中的`vector`。在项⽬ 开发中，什么时候适合⽤数组，什么时候适合⽤容器呢？

这⾥我拿`Java`语⾔来举例。如果你是Java⼯程师，⼏乎天天都在⽤`ArrayList`，对它应该⾮常熟悉。 那它与数组相⽐，到底有哪些优势呢？

我个⼈觉得，`ArrayList`最⼤的优势就是***可以将很多数组操作的细节封装起来***。⽐如前⾯提到的数组插⼊、删除数据时需要搬移其他数据等。另外，它还有⼀个优势，就是***⽀持动态扩容***。

数组本身在定义的时候需要预先指定⼤⼩，因为需要分配连续的内存空间。如果我们申请了⼤⼩为 10的数组，当第11个数据需要存储到数组中时，我们就需要重新分配⼀块更⼤的空间，将原来的数据复制过去，然后再将新的数据插⼊。

如果使⽤`ArrayList`，我们就完全不需要关⼼底层的扩容逻辑，`ArrayList`已经帮我们实现好了。每次 存储空间不够的时候，它都会将空间⾃动扩容为1.5倍⼤⼩。

不过，这⾥需要注意⼀点，因为扩容操作涉及内存申请和数据搬移，是⽐较耗时的。所以，如果事 先能确定需要存储的数据⼤⼩，最好在***创建`ArrayList`的时候事先指定数据⼤⼩***。

⽐如我们要从数据库中取出`10000`条数据放⼊`ArrayList`。我们看下⾯这⼏⾏代码，你会发现，相⽐ 之下，事先指定数据⼤⼩可以省掉很多次内存申请和数据搬移操作。

```java
ArrayList<User> users = new ArrayList(10000);
for (int i = 0; i < 10000; ++i) {
  users.add(xxx);
}
```

作为⾼级语⾔编程者，是不是数组就⽆⽤武之地了呢？当然不是，有些时候，⽤数组会更合适些， 我总结了⼏点⾃⼰的经验。

1. `Java ArrayList`⽆法存储基本类型，⽐如`int`、`long`，需要封装为`Integer`、`Long`类，⽽`Autoboxing`、 `Unboxing`则有⼀定的性能消耗，所以如果特别关注性能，或者希望使⽤基本类型，就可以选⽤数组。
2. 如果数据⼤⼩事先已知，并且对数据的操作⾮常简单，⽤不到`ArrayList`提供的⼤部分⽅法，也可以直接使⽤数组。
3. 还有⼀个是我个⼈的喜好，当要表示多维数组时，⽤数组往往会更加直观。⽐如`Object[][] array`； ⽽⽤容器的话则需要这样定义：`ArrayList<ArrayList> array`。

我总结⼀下，对于业务开发，直接使⽤容器就⾜够了，省时省⼒。毕竟损耗⼀丢丢性能，完全不会影响到系统整体的性能。但如果你是做⼀些⾮常底层的开发，⽐如开发⽹络框架，性能的优化需要 做到极致，这个时候数组就会优于容器，成为⾸选。

### 解答开篇

现在我们来思考开篇的问题：为什么⼤多数编程语⾔中，数组要从0开始编号，⽽不是从1开始呢？ 从数组存储的内存模型上来看，“下标”最确切的定义应该是“**偏移（`offset`）**”。前⾯也讲到，如果⽤a来表示数组的首地址，`a[0]`就是偏移为0的位置，`a[k]`就表示偏移`k`个`type_size`的位置，所以计算`a[k]`的内存地址只需要用到这个公式：

```java
a[k]_address = base_address + k * type_size
```

但是，如果数组从1开始计数，那我们计算数组元素a[k]的内存地址就会变为：

```java
a[k]_address = base_address + (k-1)*type_size
```

对⽐两个公式，我们不难发现，从1开始编号，每次随机访问数组元素都多了⼀次减法运算，对于`CPU`来说，就是多了⼀次减法指令。

数组作为⾮常基础的数据结构，通过下标随机访问数组元素⼜是其⾮常基础的编程操作，效率的优 化就要尽可能做到极致。所以为了减少⼀次减法操作，数组选择了从0开始编号，⽽不是从1开始。 不过我认为，上⾯解释得再多其实都算不上压倒性的证明，说数组起始编号⾮0开始不可。所以我 觉得最主要的原因可能是历史原因。

`C`语⾔设计者⽤0开始计数数组下标，之后的`Java`、`JavaScript`等⾼级语⾔都效仿了`C`语⾔，或者 说，为了在⼀定程度上减少`C`语⾔程序员学习`Java`的学习成本，因此继续沿⽤了从0开始计数的习 惯。实际上，很多语⾔中数组也并不是从0开始计数的，⽐如`Matlab`。甚⾄还有⼀些语⾔⽀持负数 下标，⽐如`Python`。

### 内容⼩结

我们今天学习了数组。它可以说是最基础、最简单的数据结构了。数组⽤⼀块连续的内存空间，来 存储相同类型的⼀组数据，最⼤的特点就是⽀持随机访问，但插⼊、删除操作也因此变得⽐较低 效，平均情况时间复杂度为`O(n)`。在平时的业务开发中，我们可以直接使⽤编程语⾔提供的容器类，但是，如果是特别底层的开发，直接使⽤数组可能会更合适。

### 课后思考

前⾯我基于数组的原理引出`JVM`的标记清除垃圾回收算法的核⼼理念。我不知道你是否使⽤`Java`语 ⾔，理解`JVM`，如果你熟悉，可以在评论区回顾下你理解的标记清除垃圾回收算法。

前⾯我们讲到⼀维数组的内存寻址公式，那你可以思考⼀下，类⽐⼀下，⼆维数组的内存寻址公式 是怎样的呢？

欢迎留⾔和我分享，我会第⼀时间给你反馈。

我已将本节内容相关的详细代码更新到`GitHub`，戳此即可查看

[GitHub - wangzheng0822/algo: 数据结构和算法必知必会的50个代码实现](https://github.com/wangzheng0822/algo)

## 06|链表（上）：如何实现LRU缓存淘汰算法?

今天我们来聊聊“链表（`Linked list`）”这个数据结构。学习链表有什么⽤呢？为了回答这个问题，我 们先来讨论⼀个经典的链表应⽤场景，那就是`LRU`缓存淘汰算法。

缓存是⼀种提⾼数据读取性能的技术，在硬件设计、软件开发中都有着⾮常⼴泛的应⽤，⽐如常⻅的`CPU`缓存、数据库缓存、浏览器缓存等等。

缓存的⼤⼩有限，当缓存被⽤满时，哪些数据应该被清理出去，哪些数据应该被保留？这就需要缓存淘汰策略来决定。常⻅的策略有三种：先进先出策略`FIFO（First In，First Out）`、最少使⽤策略 `LFU（Least Frequently Used）`、最近最少使⽤策略`LRU（Least Recently Used）`。

这些策略你不⽤死记，我打个⽐⽅你很容易就明⽩了。假如说，你买了很多本技术书，但有⼀天你 发现，这些书太多了，太占书房空间了，你要做个⼤扫除，扔掉⼀些书籍。那这个时候，你会选择 扔掉哪些书呢？对应⼀下，你的选择标准是不是和上⾯的三种策略神似呢？

好了，回到正题，我们今天的开篇问题就是：***如何⽤链表来实现`LRU`缓存淘汰策略呢？*** 带着这个问 题，我们开始今天的内容吧！

### 五花⼋⻔的链表结构

相⽐数组，链表是⼀种稍微复杂⼀点的数据结构。对于初学者来说，掌握起来也要⽐数组稍难⼀ 些。这两个⾮常基础、⾮常常⽤的数据结构，我们常常将会放到⼀块⼉来⽐较。所以我们先来看， 这两者有什么区别。

我们先从**底层的存储结构**上来看⼀看。

为了直观地对⽐，我画了⼀张图。从图中我们看到，数组需要⼀块**连续的内存空间**来存储，对内存的要求⽐较⾼。如果我们申请⼀个`100MB`⼤⼩的数组，当内存中没有连续的、⾜够⼤的存储空间时，即便内存的剩余总可⽤空间⼤于`100MB`，仍然会申请失败。

⽽链表恰恰相反，它并不需要⼀块连续的内存空间，它通过“指针”将⼀组***零散的内存块串联起来***使⽤，所以如果我们申请的是100MB⼤⼩的链表，根本不会有问题。

![](C:\Users\21133\Pictures\js\数组与链表内存分布.png)

链表结构五花⼋⻔，今天我重点给你介绍三种最常⻅的链表结构，它们分别是：**单链表**、**双向链表**和**循环链表**。我们⾸先来看最简单、最常⽤的**单链表**。

我们刚刚讲到，链表通过指针将⼀组零散的内存块串联在⼀起。其中，我们把内存块称为链表的“**结点**”。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下⼀个结点 的地址。如图所示，我们把这个记录下个结点地址的指针叫作**后继指针`next`**。

![](C:\Users\21133\Pictures\js\单链表.png)

从我画的单链表图中，你应该可以发现，其中有两个结点是⽐较特殊的，它们分别是第⼀个结点和 最后⼀个结点。我们习惯性地把第⼀个结点叫作**头结点**，把最后⼀个结点叫作**尾结点**。其中，头结 点⽤来记录链表的基地址。有了它，我们就可以遍历得到整条链表。⽽尾结点特殊的地⽅是：指针不是指向下⼀个结点，⽽是指向⼀个空地址`NULL`，表示这是链表上最后⼀个结点。

与数组⼀样，链表也⽀持数据的查找、插⼊和删除操作。

我们知道，在进⾏数组的插⼊、删除操作时，为了保持内存数据的连续性，需要做⼤量的数据搬 移，所以时间复杂度是`O(n)`。⽽在链表中插⼊或者删除⼀个数据，我们并不需要为了保持内存的连 续性⽽搬移结点，因为链表的存储空间本身就不是连续的。所以，在链表中插⼊和删除⼀个数据是⾮常快速的。

为了⽅便你理解，我画了⼀张图，从图中我们可以看出，针对链表的插⼊和删除操作，我们只需要 考虑相邻结点的指针改变，所以对应的时间复杂度是`O(1)`。

![](C:\Users\21133\Pictures\js\链表增删操作.png)

但是，有利就有弊。链表要想随机访问第`k`个元素，就没有数组那么⾼效了。因为链表中的数据并⾮连续存储的，所以⽆法像数组那样，根据⾸地址和下标，通过寻址公式就能直接计算出对应的内存 地址，⽽是需要根据指针⼀个结点⼀个结点地依次遍历，直到找到相应的结点。

你可以把链表想象成⼀个队伍，队伍中的每个⼈都只知道⾃⼰后⾯的⼈是谁，所以当我们希望知道 排在第`k`位的⼈是谁的时候，我们就需要从第⼀个⼈开始，⼀个⼀个地往下数。所以，链表随机访问 的性能没有数组好，需要`O(n)`的时间复杂度。

好了，单链表我们就简单介绍完了，接着来看另外两个复杂的升级版，**循环链表和双向链表**。 ***循环链表是⼀种特殊的单链表***。实际上，循环链表也很简单。它跟单链表唯⼀的区别就在尾结点。 我们知道，单链表的尾结点指针指向空地址，表示这就是最后的结点了。⽽循环链表的尾结点指针 是指向链表的头结点。从我画的循环链表图中，你应该可以看出来，它像⼀个环⼀样⾸尾相连，所以叫作“循环”链表。

![image-20220513005239388](C:\Users\21133\AppData\Roaming\Typora\typora-user-images\image-20220513005239388.png)

和单链表相⽐，循环链表的优点是从链尾到链头⽐较⽅便。当要处理的数据具有环型结构特点时， 就特别适合采⽤循环链表。⽐如著名的**约瑟夫问题**。尽管⽤单链表也可以实现，但是⽤循环链表实现的话，代码就会简洁很多。

单链表和循环链表是不是都不难？接下来我们再来看⼀个稍微复杂的，在实际的软件开发中，也更 加常⽤的链表结构：**双向链表**。

单向链表只有⼀个⽅向，结点只有⼀个后继指针`next`指向后⾯的结点。⽽双向链表，顾名思义，它⽀持两个⽅向，每个结点不⽌有⼀个后继指针`next`指向后⾯的结点，还有⼀个前驱指针`prev`指向前 ⾯的结点。

![image-20220513005553368](C:\Users\21133\AppData\Roaming\Typora\typora-user-images\image-20220513005553368.png)

从我画的图中可以看出来，双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。所 以，如果存储同样多的数据，双向链表要⽐单链表占⽤更多的内存空间。虽然两个指针⽐较浪费存 储空间，但可以⽀持双向遍历，这样也带来了双向链表操作的灵活性。那相⽐单链表，双向链表适合解决哪种问题呢？

从结构上来看，双向链表可以⽀持`O(1)`时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的插⼊、删除等操作都要⽐单链表简单、⾼效。

你可能会说，我刚讲到单链表的插⼊、删除操作的时间复杂度已经是O(1)了，双向链表还能再怎么 ⾼效呢？别着急，刚刚的分析⽐较偏理论，很多数据结构和算法书籍中都会这么讲，但是这种说法 实际上是不准确的，或者说是有先决条件的。我再来带你分析⼀下链表的两个操作。

我们先来看删除操作。

在实际的软件开发中，从链表中删除⼀个数据⽆外乎这两种情况：

1. 删除结点中“值等于某个给定值”的结点；
2. 删除给定指针指向的结点。

对于第⼀种情况，不管是单链表还是双向链表，为了查找到值等于给定值的结点，都需要从头结点 开始⼀个⼀个依次遍历对⽐，直到找到值等于给定值的结点，然后再通过我前⾯讲的指针操作将其删除。

尽管单纯的删除操作时间复杂度是`O(1)`，但遍历查找的时间是主要的耗时点，对应的时间复杂度为`O(n)`。根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为`O(n)`。

对于第⼆种情况，我们已经找到了要删除的结点，但是删除某个结点`q`需要知道其前驱结点，⽽单链表并不⽀持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到`p->next=q`，说明`p`是`q`的前驱结点。

但是对于双向链表来说，这种情况就⽐较有优势了。因为双向链表中的结点已经保存了前驱结点的 指针，不需要像单链表那样遍历。所以，针对第⼆种情况，单链表删除操作需要`O(n)`的时间复杂 度，⽽双向链表只需要在`O(1)`的时间复杂度内就搞定了！

同理，如果我们希望在链表的某个指定结点前⾯插⼊⼀个结点，双向链表⽐单链表有很⼤的优势。 双向链表可以在`O(1)`时间复杂度搞定，⽽单向链表需要`O(n)`的时间复杂度。你可以参照我刚刚讲过的删除操作⾃⼰分析⼀下。

除了插⼊、删除操作有优势之外，对于⼀个有序链表，双向链表的按值查询的效率也要⽐单链表⾼⼀些。因为，我们可以记录上次查找的位置p，每次查询时，根据要查找的值与`p`的⼤⼩关系，决定 是往前还是往后查找，所以平均只需要查找⼀半的数据。

现在，你有没有觉得双向链表要⽐单链表更加⾼效呢？这就是为什么在实际的软件开发中，双向链表尽管⽐较费内存，但还是⽐单链表的应⽤更加⼴泛的原因。如果你熟悉`Java`语⾔，你肯定⽤过 `LinkedHashMap`这个容器。如果你深⼊研究`LinkedHashMap`的实现原理，就会发现其中就⽤到了双 向链表这种数据结构。

实际上，这⾥有⼀个更加重要的知识点需要你掌握，那就是**⽤空间换时间**的设计思想。当内存空间充⾜的时候，如果我们更加追求代码的执⾏速度，我们就可以选择空间复杂度相对较⾼、但时间复 杂度相对很低的算法或者数据结构。相反，如果内存⽐较紧缺，⽐如代码跑在⼿机或者单⽚机上， 这个时候，就要反过来⽤时间换空间的设计思路。

还是开篇缓存的例⼦。缓存实际上就是利⽤了空间换时间的设计思想。如果我们把数据存储在硬盘上，会⽐较节省内存，但每次查找数据都要询问⼀次硬盘，会⽐较慢。但如果我们通过缓存技术， 事先将数据加载在内存中，虽然会⽐较耗费内存空间，但是每次数据查询的速度就⼤⼤提⾼了。 所以我总结⼀下，对于执⾏较慢的程序，可以通过消耗更多的内存（空间换时间）来进⾏优化；⽽ 消耗过多内存的程序，可以通过消耗更多的时间（时间换空间）来降低内存的消耗。你还能想到其 他时间换空间或者空间换时间的例⼦吗？

了解了循环链表和双向链表，如果把这两种链表整合在⼀起就是⼀个新的版本：**双向循环链表**。我 想不⽤我多讲，你应该知道双向循环链表⻓什么样⼦了吧？你可以⾃⼰试着在纸上画⼀画。

![image-20220513010123280](C:\Users\21133\AppData\Roaming\Typora\typora-user-images\image-20220513010123280.png)

### 链表VS数组性能⼤⽐拼

通过前⾯内容的学习，你应该已经知道，数组和链表是两种截然不同的内存组织⽅式。正是因为内 存存储的区别，它们插⼊、删除、随机访问操作的时间复杂度正好相反。

![image-20220513010217017](C:\Users\21133\AppData\Roaming\Typora\typora-user-images\image-20220513010217017.png)

不过，数组和链表的对⽐，并不能局限于时间复杂度。⽽且，在实际的软件开发中，不能仅仅利⽤ 复杂度分析就决定使⽤哪个数据结构来存储数据。

数组简单易⽤，在实现上使⽤的是连续的内存空间，可以借助`CPU`的缓存机制，预读数组中的数 据，所以访问效率更⾼。⽽链表在内存中并不是连续存储，所以对`CPU`缓存不友好，没办法有效预读。

数组的缺点是⼤⼩固定，⼀经声明就要占⽤整块连续内存空间。如果声明的数组过⼤，系统可能没 有⾜够的连续内存空间分配给它，导致“内存不⾜（`out of memory`）”。如果声明的数组过⼩，则可 能出现不够⽤的情况。这时只能再申请⼀个更⼤的内存空间，把原数组拷⻉进去，⾮常费时。链表 本身没有⼤⼩的限制，天然地⽀持动态扩容，我觉得这也是它与数组最⼤的区别。

你可能会说，我们`Java`中的`ArrayList`容器，也可以⽀持动态扩容啊？我们上⼀节课讲过，当我们往 ⽀持动态扩容的数组中插⼊⼀个数据时，如果数组中没有空闲空间了，就会申请⼀个更⼤的空间， 将数据拷⻉过去，⽽数据拷⻉的操作是⾮常耗时的。

我举⼀个稍微极端的例⼦。如果我们⽤`ArrayList`存储了了`1GB`⼤⼩的数据，这个时候已经没有空闲 空间了，当我们再插⼊数据的时候，`ArrayList`会申请⼀个`1.5GB`⼤⼩的存储空间，并且把原来那 `1GB`的数据拷⻉到新申请的空间上。听起来是不是就很耗时？

除此之外，如果你的代码对内存的使⽤⾮常苛刻，那数组就更适合你。因为链表中的每个结点都需 要消耗额外的存储空间去存储⼀份指向下⼀个结点的指针，所以内存消耗会翻倍。⽽且，对链表进 ⾏频繁的插⼊、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎⽚，如果是`Java`语⾔，就有可能会导致频繁的`GC`（`Garbage Collection`，垃圾回收）。

所以，在我们实际的开发中，针对不同类型的项⽬，要根据具体情况，权衡究竟是选择数组还是链 表。

### 解答开篇

好了，关于链表的知识我们就讲完了。我们现在回过头来看下开篇留给你的思考题。如何基于链表 实现`LRU`缓存淘汰算法？

我的思路是这样的：我们维护⼀个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有⼀个新的数据被访问时，我们从链表头开始顺序遍历链表。

1. 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位 置删除，然后再插⼊到链表的头部。
2. 如果此数据没有在缓存链表中，⼜可以分为两种情况：如果此时缓存未满，则将此结点直接插⼊到链表的头部；如果此时缓存已满，则链表尾结点删除，将新的数据结点插⼊链表的头部。

这样我们就⽤链表实现了⼀个`LRU`缓存，是不是很简单？

现在我们来看下`m`缓存访问的时间复杂度是多少。因为不管缓存有没有满，我们都需要遍历⼀遍链 表，所以这种基于链表的实现思路，缓存访问的时间复杂度为`O(n)`。

实际上，我们可以继续优化这个实现思路，⽐如引⼊散列表（`Hash table`）来记录每个数据的位 置，将缓存访问的时间复杂度降到`O(1)`。因为要涉及我们还没有讲到的数据结构，所以这个优化⽅ 案，我现在就不详细说了，等讲到散列表的时候，我会再拿出来讲。

除了基于链表的实现思路，实际上还可以⽤数组来实现LRU缓存淘汰策略。如何利⽤数组实现LRU 缓存淘汰策略呢？我把这个问题留给你思考。

### 内容⼩结

今天我们讲了⼀种跟数组“相反”的数据结构，链表。它跟数组⼀样，也是⾮常基础、⾮常常⽤的数 据结构。不过链表要⽐数组稍微复杂，从普通的单链表衍⽣出来好⼏种链表结构，⽐如双向链表、 循环链表、双向循环链表。

和数组相⽐，链表更适合插⼊、删除操作频繁的场景，查询的时间复杂度较⾼。不过，在具体软件 开发中，要对数组和链表的各种性能进⾏对⽐，综合来选择使⽤两者中的哪⼀个。

### 课后思考

如何判断⼀个字符串是否是回⽂字符串的问题，我想你应该听过，我们今天的思题⽬就是基于这个 问题的改造版本。如果字符串是通过单链表来存储的，那该如何来判断是⼀个回⽂串呢？你有什么 好的解决思路呢？相应的时间空间复杂度⼜是多少呢？

## 如何轻松写出正确的链表代码?

上⼀节我讲了链表相关的基础知识。学完之后，我看到有⼈留⾔说，基础知识我都掌握了，但是写 链表代码还是很费劲。哈哈，的确是这样的！

想要写好链表代码并不是容易的事⼉，尤其是那些复杂的链表操作，⽐如链表反转、有序链表合并 等，写的时候⾮常容易出错。从我上百场⾯试的经验来看，能把“链表反转”这⼏⾏代码写对的⼈不⾜10%。

为什么链表代码这么难写？究竟怎样才能⽐较轻松地写出正确的链表代码呢？

只要愿意投⼊时间，我觉得⼤多数⼈都是可以学会的。⽐如说，如果你真的能花上⼀个周末或者⼀ 整天的时间，就去写链表反转这⼀个代码，多写⼏遍，⼀直练到能毫不费⼒地写出`Bug free`的代 码。这个坎还会很难跨吗？

当然，⾃⼰有决⼼并且付出精⼒是成功的先决条件，除此之外，我们还需要⼀些⽅法和技巧。我根 据⾃⼰的学习经历和⼯作经验，总结了***⼏个写链表代码技巧***。如果你能熟练掌握这⼏个技巧，加上 你的主动和坚持，轻松拿下链表代码完全没有问题。 

### 技巧⼀：理解指针或引⽤的含义

事实上，看懂链表的结构并不是很难，但是⼀旦把它和指针混在⼀起，就很容易让⼈摸不着头脑。 所以，要想写对链表代码，⾸先就要理解好指针。

我们知道，有些语⾔有“指针”的概念，⽐如C语⾔；有些语⾔没有指针，取⽽代之的是“引⽤”，⽐如`Java`、`Python`。不管是“指针”还是“引⽤”，实际上，它们的意思都是⼀样的，都是存储所指对象的内 存地址。

接下来，我会拿`C`语⾔中的“指针”来讲解，如果你⽤的是`Java`或者其他没有指针的语⾔也没关系，你 把它理解成“引⽤”就可以了。

实际上，对于指针的理解，你只需要记住下⾯这句话就可以了：***将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了 这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。***

这句话听起来还挺拗⼝的，你可以先记住。我们回到链表代码的编写过程中，我来慢慢给你解释。 在编写链表代码的时候，我们经常会有这样的代码：`p->next=q`。这⾏代码是说，`p`结点中的`next`指 针存储了`q`结点的内存地址。

还有⼀个更复杂的，也是我们写链表代码经常会⽤到的：`p->next=p->next->next`。这⾏代码表 示，`p`结点的`next`指针存储了`p`结点的下下⼀个结点的内存地址。

掌握了指针或引⽤的概念，你应该可以很轻松地看懂链表代码。恭喜你，已经离写出链表代码近了 ⼀步！

### 技巧⼆：警惕指针丢失和内存泄漏

不知道你有没有这样的感觉，写链表代码的时候，指针指来指去，⼀会⼉就不知道指到哪⾥了。所 以，我们在写的时候，⼀定注意不要弄丢了指针。

指针往往都是怎么弄丢的呢？我拿单链表的插⼊操作为例来给你分析⼀下。

![image-20220513100834251](C:\Users\21133\AppData\Roaming\Typora\typora-user-images\image-20220513100834251.png)

如图所示，我们希望在结点`a`和相邻的结点`b`之间插⼊结点`x`，假设当前指针`p`指向结点`a`。如果我们 将代码实现变成下⾯这个样⼦，就会发⽣指针丢失和内存泄露。

```c
p->next = x; // 将p的next指针指向x结点；
x->next = p->next; // 将x的结点的next指针指向b结点；
```

初学者经常会在这⼉犯错。`p->next`指针在完成第⼀步操作之后，已经不再指向结点`b`了，⽽是指向结点`x`。第2⾏代码相当于将x赋值给`x->next`，⾃⼰指向⾃⼰。因此，整个链表也就断成了两半，从 结点`b`往后的所有结点都⽆法访问到了。

对于有些语⾔来说，⽐如`C`语⾔，内存管理是由程序员负责的，如果没有⼿动释放结点对应的内存空间，就会产⽣内存泄露。所以，我们***插⼊结点时，⼀定要注意操作的顺序***，要先将结点`x`的`next`指 针指向结点`b`，再把结点`a`的`next`指针指向结点`x`，这样才不会丢失指针，导致内存泄漏。所以，对于 刚刚的插⼊代码，我们只需要把第1⾏和第2⾏代码的顺序颠倒⼀下就可以了。

同理，***删除链表结点时，也⼀定要记得⼿动释放内存空间***，否则，也会出现内存泄漏的问题。当 然，对于像Java这种虚拟机⾃动管理内存的编程语⾔来说，就不需要考虑这么多了。 

### 技巧三：利⽤哨兵简化实现难度

⾸先，我们先来回顾⼀下单链表的插⼊和删除操作。如果我们在结点`p`后⾯插⼊⼀个新的结点，只 需要下⾯两⾏代码就可以搞定。

```c
new_node->next = p->next;
p->next = new_node;
```

但是，当我们要向⼀个空链表中插⼊第⼀个结点，刚刚的逻辑就不能⽤了。我们需要进⾏下⾯这样 的特殊处理，其中`head`表示链表的头结点。所以，从这段代码，我们可以发现，对于单链表的插⼊ 操作，第⼀个结点和其他结点的插⼊逻辑是不⼀样的。

```c
if (head == null) {
  head = new_node;
}
```

我们再来看单链表结点删除操作。如果要删除结点`p`的后继结点，我们只需要⼀⾏代码就可以搞定。

```c
p->next = p->next->next;
```

但是，如果我们要删除链表中的最后⼀个结点，前⾯的删除代码就不work了。跟插⼊类似，我们也 需要对于这种情况特殊处理。写成代码是这样⼦的：

```c
if (head->next == null) {
  head = null;
}
```

从前⾯的⼀步⼀步分析，我们可以看出，***针对链表的插⼊、删除操作，需要对插⼊第⼀个结点和删除最后⼀个结点的情况进⾏特殊处理***。这样代码实现起来就会很繁琐，不简洁，⽽且也容易因为考 虑不全⽽出错。如何来解决这个问题呢？

技巧三中提到的哨兵就要登场了。哨兵，解决的是国家之间的边界问题。同理，这⾥说的哨兵也是 解决“边界问题”的，不直接参与业务逻辑。

还记得如何表示⼀个空链表吗？`head=null`表示链表中没有结点了。其中`head`表示头结点指针，指向链表中的第⼀个结点。

如果我们引⼊哨兵结点，在任何时候，不管链表是不是空，`head`指针都会⼀直指向这个哨兵结点。 我们也把这种有哨兵结点的链表叫**带头链表**。相反，没有哨兵结点的链表就叫作**不带头链表**。

我画了⼀个带头链表，你可以发现，哨兵结点是不存储数据的。因为哨兵结点⼀直存在，所以插⼊ 第⼀个结点和插⼊其他结点，删除最后⼀个结点和删除其他结点，都可以统⼀为相同的代码实现逻辑了。

![image-20220513101749785](C:\Users\21133\AppData\Roaming\Typora\typora-user-images\image-20220513101749785.png)

实际上，这种利⽤哨兵简化编程难度的技巧，在很多代码实现中都有⽤到，⽐如插⼊排序、归并排 序、动态规划等。这些内容我们后⾯才会讲，现在为了让你感受更深，我再举⼀个⾮常简单的例⼦。代码我是⽤`C`语⾔实现的，不涉及语⾔⽅⾯的⾼级语法，很容易看懂，你可以类⽐到你熟悉的语⾔。

代码⼀：

```c
// 在数组a中，查找key，返回key所在的位置
// 其中，n表示数组a的⻓度
int find(char* a, int n, char key) {
// 边界条件处理，如果a为空，或者n<=0，说明数组中没有数据，就不⽤while循环⽐较了
  if(a == null || n <= 0) {
    return -1;
  }
  int i = 0;
// 这⾥有两个⽐较操作：i<n和a[i]==key.
  while (i < n) {
    if (a[i] == key) {
      return i;
    }
    ++i;
  }
  return -1;
}
```

代码⼆：

```c
// 在数组a中，查找key，返回key所在的位置
// 其中，n表示数组a的⻓度
// 我举2个例⼦，你可以拿例⼦⾛⼀下代码
// a = {4, 2, 3, 5, 9, 6} n=6 key = 7
// a = {4, 2, 3, 5, 9, 6} n=6 key = 6
int find(char* a, int n, char key) {
  if(a == null || n <= 0) {
	return -1;
  }
// 这⾥因为要将a[n-1]的值替换成key，所以要特殊处理这个值
  if (a[n-1] == key) {
	return n-1;
  }
// 把a[n-1]的值临时保存在变量tmp中，以便之后恢复。tmp=6。
// 之所以这样做的⽬的是：希望find()代码不要改变a数组中的内容
  char tmp = a[n-1];
// 把key的值放到a[n-1]中，此时a = {4, 2, 3, 5, 9, 7}
  a[n-1] = key;
  int i = 0;
// while 循环⽐起代码⼀，少了i<n这个⽐较操作
  while (a[i] != key) {
	++i;
  }
// 恢复a[n-1]原来的值,此时a= {4, 2, 3, 5, 9, 6}
  a[n-1] = tmp;
  if (i == n-1) {
    // 如果i == n-1说明，在0...n-2之间都没有key，所以返回-1
    return -1;
  } else {
	// 否则，返回i，就是等于key值的元素的下标
	return i;
  }
}

```

当然，这只是为了举例说明哨兵的作⽤，你写代码的时候千万不要写第⼆段那样的代码，因为可读 性太差了。⼤部分情况下，我们并不需要如此追求极致的性能。

### 技巧四：重点留意边界条件处理

软件开发中，代码在⼀些边界或者异常情况下，最容易产⽣`Bug`。链表代码也不例外。要实现没有`Bug`的链表代码，⼀定要在编写的过程中以及编写完成之后，检查边界条件是否考虑全⾯，以及代 码在边界条件下是否能正确运⾏。

我经常⽤来检查链表代码是否正确的边界条件有这样⼏个：

1. 如果链表为空时，代码是否能正常⼯作？
2. 如果链表只包含⼀个结点时，代码是否能正常⼯作？
3. 如果链表只包含两个结点时，代码是否能正常⼯作？
4. 代码逻辑在处理头结点和尾结点的时候，是否能正常⼯作？
5. 当你写完链表代码之后，除了看下你写的代码在正常的情况下能否⼯作，还要看下在上⾯我列举的 ⼏个边界条件下，代码仍然能否正确⼯作。如果这些边界条件下都没有问题，那基本上可以认为没 有问题了。

当然，边界条件不⽌我列举的那些。针对不同的场景，可能还有特定的边界条件，这个需要你⾃⼰ 去思考，不过套路都是⼀样的。

实际上，不光光是写链表代码，你在写任何代码时，也千万不要只是实现业务正常情况下的功能就 好了，⼀定要多想想，你的代码在运⾏的时候，可能会遇到哪些边界情况或者异常情况。遇到了应 该如何应对，这样写出来的代码才够健壮！

### 技巧五：举例画图，辅助思考

对于稍微复杂的链表操作，⽐如前⾯我们提到的单链表反转，指针⼀会⼉指这，⼀会⼉指那，⼀会 ⼉就被绕晕了。总感觉脑容量不够，想不清楚。所以这个时候就要使⽤⼤招了，**举例法**和**画图法**。 你可以找⼀个具体的例⼦，把它画在纸上，释放⼀些脑容量，留更多的给逻辑思考，这样就会感觉 到思路清晰很多。⽐如往单链表中插⼊⼀个数据这样⼀个操作，我⼀般都是把各种情况都举⼀个例 ⼦，画出插⼊前和插⼊后的链表变化，如图所示：

![image-20220513110030117](C:\Users\21133\AppData\Roaming\Typora\typora-user-images\image-20220513110030117.png)

### 技巧六：多写多练，没有捷径

如果你已经理解并掌握了我前⾯所讲的⽅法，但是⼿写链表代码还是会出现各种各样的错误，也不 要着急。因为我最开始学的时候，这种状况也持续了⼀段时间。

现在我写这些代码，简直就和“玩⼉”⼀样，其实也没有什么技巧，就是把常⻅的链表操作都⾃⼰多 写⼏遍，出问题就⼀点⼀点调试，熟能⽣巧！

所以，我精选了5个常⻅的链表操作。你只要把这⼏个操作都能写熟练，不熟就多写⼏遍，我保证 你之后再也不会害怕写链表代码。

1. 单链表反转
2. 链表中环的检测
3. 两个有序的链表合并
4. 删除链表倒数第n个结点
5. 求链表的中间结点

### 内容⼩结

这节我主要和你讲了写出正确链表代码的六个技巧。分别是理解指针或引⽤的含义、警惕指针丢失 和内存泄漏、利⽤哨兵简化实现难度、重点留意边界条件处理，以及举例画图、辅助思考，还有多 写多练。

我觉得，写链表代码是最考验逻辑思维能⼒的。因为，链表代码到处都是指针的操作、边界条件的 处理，稍有不慎就容易产⽣Bug。链表代码写得好坏，可以看出⼀个⼈写代码是否够细⼼，考虑问 题是否全⾯，思维是否缜密。所以，这也是很多⾯试官喜欢让⼈⼿写链表代码的原因。所以，这⼀ 节讲到的东⻄，你⼀定要⾃⼰写代码实现⼀下，才有效果。 

### 课后思考

今天我们讲到⽤哨兵来简化编码实现，你是否还能够想到其他场景，利⽤哨兵可以⼤⼤地简化编码 难度？

欢迎留⾔和我分享，我会第⼀时间给你反馈。

## 08|栈：如何实现浏览器的前进和后退功能?

浏览器的前进、后退功能，我想你肯定很熟悉吧？

当你依次访问完⼀串⻚⾯`a-b-c`之后，点击浏览器的后退按钮，就可以查看之前浏览过的⻚⾯`b`和`a`。 当你后退到⻚⾯`a`，点击前进按钮，就可以重新查看⻚⾯`b`和`c`。但是，如果你后退到⻚⾯`b`后，点击 了新的⻚⾯`d`，那就⽆法再通过前进、后退功能查看⻚⾯`c`了。

***假设你是Chrome浏览器的开发⼯程师，你会如何实现这个功能呢？***

这就要⽤到我们今天要讲的“栈”这种数据结构。带着这个问题，我们来学习今天的内容。

### 如何理解“栈”？

关于“栈”，我有⼀个⾮常贴切的例⼦，就是⼀摞叠在⼀起的盘⼦。我们平时放盘⼦的时候，都是从 下往上⼀个⼀个放；取的时候，我们也是从上往下⼀个⼀个地依次取，不能从中间任意抽出。***后进者先出，先进者后出，这就是典型的“栈”结构。***

![image-20220626152609781](C:\Users\21133\AppData\Roaming\Typora\typora-user-images\image-20220626152609781.png)

从栈的操作特性上来看，栈是⼀种“操作受限”的线性表，只允许在⼀端插⼊和删除数据。

我第⼀次接触这种数据结构的时候，就对它存在的意义产⽣了很⼤的疑惑。因为我觉得，相⽐数组 和链表，栈带给我的只有限制，并没有任何优势。那我直接使⽤数组或者链表不就好了吗？为什么 还要⽤这个“操作受限”的“栈”呢？

事实上，从功能上来说，数组或链表确实可以替代栈，但你要知道，特定的数据结构是对特定场景 的抽象，⽽且，数组或链表暴露了太多的操作接⼝，操作上的确灵活⾃由，但使⽤时就⽐较不可 控，⾃然也就更容易出错。

***当某个数据集合只涉及在⼀端插⼊和删除数据，并且满⾜后进先出、先进后出的特性，我们就应该 ⾸选“栈”这种数据结构。***

### 如何实现⼀个“栈”？

从刚才栈的定义⾥，我们可以看出，栈主要包含两个操作，⼊栈和出栈，也就是在栈顶插⼊⼀个数 据和从栈顶删除⼀个数据。理解了栈的定义之后，我们来看⼀看如何⽤代码实现⼀个栈。

实际上，栈既可以⽤数组来实现，也可以⽤链表来实现。⽤数组实现的栈，我们叫作**顺序栈**，⽤链 表实现的栈，我们叫作**链式栈**。

我这⾥实现⼀个基于数组的顺序栈。基于链表实现的链式栈的代码，你可以⾃⼰试着写⼀下。我会 将我写好的代码放到Github上，你可以去看⼀下⾃⼰写的是否正确。

我这段代码是⽤`Java`来实现的，但是不涉及任何⾼级语法，并且我还⽤中⽂做了详细的注释，所以 你应该是可以看懂的。

```java
// 基于数组实现的顺序栈
plublic class ArrayStack {
    private String[] item;  // 数组
    private int count;      // 栈中元素个数
    private int n;          // 栈的大小
   
    // 初始化数组，申请一个大小为n的数组空间
    public ArrayStack(int n) {
      this.items = new String[n];
      this.n = n;
      this.count = 0;
    }
    
    // 入栈操作
    public boolean push(String item) {
      // 数组空间不够了，直接返回false，⼊栈失败。
      if (count == n) return false;
      // 将item放到下标为count的位置，并且count加⼀
      items[count] = item;
      ++count;
      return true;
    }
    
    // 出栈操作
    public String pop() {
      // 栈为空，则直接返回null
      if (count == 0) return null;
      // 返回下标为count-1的数组元素，并且栈中元素个数count减⼀
      String tmp = items[count-1];
      --count;
      return tmp;
    }
}
```

了解了定义和基本操作，那它的操作的时间、空间复杂度是多少呢？ 不管是顺序栈还是链式栈，我们存储数据只需要⼀个⼤⼩为`n`的数组就够了。在⼊栈和出栈过程 中，只需要⼀两个临时变量存储空间，所以空间复杂度是`O(1)`。

注意，这⾥存储数据需要⼀个⼤⼩为`n`的数组，并不是说空间复杂度就是`O(n)`。因为，这n个空间是 必须的，⽆法省掉。所以我们说空间复杂度的时候，是指除了原本的数据存储空间外，算法运⾏还 需要额外的存储空间。

空间复杂度分析是不是很简单？时间复杂度也不难。不管是顺序栈还是链式栈，⼊栈、出栈只涉及栈顶个别数据的操作，所以时间复杂度都是`O(1)`。

### ⽀持动态扩容的顺序栈

刚才那个基于数组实现的栈，是⼀个固定⼤⼩的栈，也就是说，在初始化栈时需要事先指定栈的⼤⼩。当栈满之后，就⽆法再往栈⾥添加数据了。尽管链式栈的⼤⼩不受限，但要存储`next`指针，内存消耗相对较多。那我们如何基于数组实现⼀个可以⽀持动态扩容的栈呢？

你还记得，我们在数组那⼀节，是如何来实现⼀个⽀持动态扩容的数组的吗？当数组空间不够时， 我们就重新申请⼀块更⼤的内存，将原来数组中数据统统拷⻉过去。这样就实现了⼀个⽀持动态扩容的数组。

所以，如果要实现⼀个⽀持动态扩容的栈，我们只需要底层依赖⼀个⽀持动态扩容的数组就可以 了。当栈满了之后，我们就申请⼀个更⼤的数组，将原来的数据搬移到新数组中。我画了⼀张图， 你可以对照着理解⼀下

![image-20220626154115255](C:\Users\21133\AppData\Roaming\Typora\typora-user-images\image-20220626154115255.png)

实际上，⽀持动态扩容的顺序栈，我们平时开发中并不常⽤到。我讲这⼀块的⽬的，主要还是希望 带你练习⼀下前⾯讲的复杂度分析⽅法。所以这⼀⼩节的重点是复杂度分析。

你不⽤死记硬背⼊栈、出栈的时间复杂度，你需要掌握的是分析⽅法。能够⾃⼰分析才算是真正掌 握了。现在我就带你分析⼀下⽀持动态扩容的顺序栈的⼊栈、出栈操作的时间复杂度。

对于出栈操作来说，我们不会涉及内存的重新申请和数据的搬移，所以出栈的时间复杂度仍然是 O(1)。但是，对于⼊栈操作来说，情况就不⼀样了。当栈中有空闲空间时，⼊栈操作的时间复杂度 为O(1)。但当空间不够时，就需要重新申请内存和数据搬移，所以时间复杂度就变成了O(n)。

也就是说，对于⼊栈操作来说，最好情况时间复杂度是O(1)，最坏情况时间复杂度是O(n)。那平均 情况下的时间复杂度⼜是多少呢？还记得我们在复杂度分析那⼀节中讲的摊还分析法吗？这个⼊栈 操作的平均情况下的时间复杂度可以⽤摊还分析法来分析。我们也正好借此来实战⼀下摊还分析法。

为了分析的⽅便，我们需要事先做⼀些假设和定义：

1. 栈空间不够时，我们重新申请⼀个是原来⼤⼩两倍的数组；
2. 为了简化分析，假设只有⼊栈操作没有出栈操作；
3. 定义不涉及内存搬移的⼊栈操作为`simple-push`操作，时间复杂度为O(1)。
4. 如果当前栈⼤⼩为`K`，并且已满，当再有新的数据要⼊栈时，就需要重新申请`2`倍⼤⼩的内存，并且 做K个数据的搬移操作，然后再⼊栈。但是，接下来的`K-1`次⼊栈操作，我们都不需要再重新申请内 存和搬移数据，所以这`K-1`次⼊栈操作都只需要⼀个`simple-push`操作就可以完成。

为了让你更加直 观地理解这个过程，我画了⼀张图。

![image-20220626154421024](C:\Users\21133\AppData\Roaming\Typora\typora-user-images\image-20220626154421024.png)

你应该可以看出来，这`K`次⼊栈操作，总共涉及了K个数据的搬移，以及`K`次`simple-push`操作。将`K`个数据搬移均摊到`K`次⼊栈操作，那每个⼊栈操作只需要⼀个数据搬移和⼀个`simple-push`操作。以 此类推，⼊栈操作的均摊时间复杂度就为`O(1)`。

通过这个例⼦的实战分析，也印证了前⾯讲到的，均摊时间复杂度⼀般都等于最好情况时间复杂度。因为在⼤部分情况下，⼊栈操作的时间复杂度`O`都是`O(1)`，只有在个别时刻才会退化为`O(n)`， 所以把耗时多的⼊栈操作的时间均摊到其他⼊栈操作上，平均情况下的耗时就接近`O(1)`。 

### 栈在函数调⽤中的应⽤

前⾯我讲的都⽐较偏理论，我们现在来看下，栈在软件⼯程中的实际应⽤。栈作为⼀个⽐较基础的 数据结构，应⽤场景还是蛮多的。其中，⽐较经典的⼀个应⽤场景就是**函数调⽤栈**。

我们知道，操作系统给每个线程分配了⼀块独⽴的内存空间，这块内存被组织成“栈”这种结构,⽤来 存储函数调⽤时的临时变量。每进⼊⼀个函数，就会将临时变量作为⼀个栈帧⼊栈，当被调⽤函数执⾏完成，返回之后，将这个函数对应的栈帧出栈。为了让你更好地理解，我们⼀块来看下这段代码的执⾏过程。

```java
int main() {
  int a = 1;
  int ret = 0;
  int res = 0;
  ret = add(3, 5);
  res = a + ret;
  printf("%d", res);
  reuturn 0;
}

int add(int x, int y) {
  int sum = 0;
  sum = x + y;
  return sum;
}

```

从代码中我们可以看出，`main()`函数调⽤了`add()`函数，获取计算结果，并且与临时变量`a`相加，最 后打印`res`的值。为了让你清晰地看到这个过程对应的函数栈⾥出栈、⼊栈的操作，我画了⼀张图。 图中显示的是，在执⾏到`add()`函数时，函数调⽤栈的情况。

![image-20220626155339664](C:\Users\21133\AppData\Roaming\Typora\typora-user-images\image-20220626155339664.png)

### 栈在表达式求值中的应⽤

我们再来看栈的另⼀个常⻅的应⽤场景，编译器如何利⽤栈来实现**表达式求值**。

为了⽅便解释，我将算术表达式简化为只包含加减乘除四则运算，⽐如：`34+13*9+44-12/3`。对于 这个四则运算，我们⼈脑可以很快求解出答案，但是对于计算机来说，理解这个表达式本身就是个挺难的事⼉。如果换作你，让你来实现这样⼀个表达式求值的功能，你会怎么做呢？

实际上，编译器就是通过两个栈来实现的。其中⼀个保存操作数的栈，另⼀个是保存运算符的栈。 我们从左向右遍历表达式，当遇到数字，我们就直接压⼊操作数栈；当遇到运算符，就与运算符栈的栈顶元素进⾏⽐较。

如果⽐运算符栈顶元素的优先级⾼，就将当前运算符压⼊栈；如果⽐运算符栈顶元素的优先级低或 者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取2个操作数，然后进⾏计算，再把计算完的结果压⼊操作数栈，继续⽐较。

我将`3+5*8-6`这个表达式的计算过程画成了⼀张图，你可以结合图来理解我刚讲的计算过程。

![image-20220626155621383](C:\Users\21133\AppData\Roaming\Typora\typora-user-images\image-20220626155621383.png)

这样⽤两个栈来解决的思路是不是⾮常巧妙？你有没有想到呢？
